{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Productionising the ML model\n",
    "This notebook will walk through the process of creating, building and commiting the artifacts required to run the model developed in the Experimentation Notebook in production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "**NOTE:** Set Project ID to your project  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for demokfp-cluster.\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'demokfp'\n",
    "PREFIX = PROJECT_ID\n",
    "REGION = 'us-central1'\n",
    "\n",
    "DATA_ROOT = 'gs://workshop-datasets/covertype'\n",
    "TRAINING_FILE_PATH = DATA_ROOT + '/training/dataset.csv'\n",
    "VALIDATION_FILE_PATH = DATA_ROOT + '/evaluation/dataset.csv'\n",
    "\n",
    "# Job dir for AI Platform Training\n",
    "JOB_DIR_ROOT='gs://{}-artifact-store/jobs'.format(PREFIX)\n",
    "\n",
    "\n",
    "NAMESPACE='kubeflow'\n",
    "ZONE='us-central1-a'\n",
    "ARTIFACT_STORE_URI='gs://{}-artifact-store'.format(PREFIX)\n",
    "GCS_STAGING_PATH='{}/staging'.format(ARTIFACT_STORE_URI)\n",
    "GKE_CLUSTER_NAME='{}-cluster'.format(PREFIX)\n",
    "\n",
    "!gcloud container clusters get-credentials $GKE_CLUSTER_NAME --zone $ZONE\n",
    "HOST_TEMP=!(kubectl describe configmap inverse-proxy-config -n $NAMESPACE | grep \"googleusercontent.com\")\n",
    "INVERSE_PROXY_HOSTNAME=HOST_TEMP[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data set to BQ\n",
    "Import the data set from cloud storage to BigQuery. A dataset is created and the table is imported under `covertype_data.covertype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Dataset 'demokfp:covertype_dataset' already\n",
      "exists.\n",
      "Waiting on bqjob_re662610dab0ecf4_000001703771d97c_1 ... (9s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "DATASET_LOCATION='US'\n",
    "DATASET_ID='covertype_dataset'\n",
    "TABLE_ID='covertype'\n",
    "DATA_SOURCE='gs://workshop-datasets/covertype/full/dataset.csv'\n",
    "SCHEMA='Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER'\n",
    "\n",
    "!bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "!bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training application.\n",
    "Now that the data is hosted in BQ, the next step is to create the training application. Start by creating the folders to host the model script, trainer image docker and the base image docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'trainer_image'\n",
    "BASE_IMAGE_FOLDER='base_image'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)\n",
    "os.makedirs(BASE_IMAGE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the training script. \n",
    "\n",
    "The script written in the Experimentation Notebook which process the data and trains the classification model is written as a training script `train.py` in the training image folder. In addition to the model written during experimentation an additional `hypertune` function is created which allows for a training job to be run with multiple parameters. This will  run multiple models with a range of parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\"\"\"Covertype Classifier trainer script.\"\"\"\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "  df_train = pd.read_csv(training_dataset_path)\n",
    "  df_validation = pd.read_csv(validation_dataset_path)\n",
    "    \n",
    "  if not hptune:\n",
    "    df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "  numeric_feature_indexes = slice(0, 10)\n",
    "  categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "  preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "  pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "    \n",
    "  num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "  df_train = df_train.astype(num_features_type_map)\n",
    "  df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "  print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "  X_train = df_train.drop('Cover_Type', axis=1)\n",
    "  y_train = df_train['Cover_Type']\n",
    "  \n",
    "  pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  \n",
    "  if hptune:\n",
    "    X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "    y_validation = df_validation['Cover_Type']\n",
    "    accuracy = pipeline.score(X_validation, y_validation)\n",
    "    print('Model accuracy: {}'.format(accuracy))\n",
    "    # Log it with hypertune\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='accuracy',\n",
    "      metric_value=accuracy\n",
    "    )\n",
    "\n",
    "  # Save the model\n",
    "  if not hptune:\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "    print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "The docker images used for the training are based off the image `mlops-dev:TF115-TFX015-KFP136` created during the inital set up of the environment. Since the AI Platform Notebook instance is based on the `mlops-dev:TF115-TFX015-KFP136` image we use the same image as a base for the training image. \n",
    "\n",
    "\n",
    "We first write a base image dockerfile which replicates the image used for the Notebook. Then we write a training dockerfile which uses the same base image and add the `train.py` to the image. \n",
    "\n",
    "\n",
    "**NOTE:** Make sure to update the URI for the image so that it points to your project's **Container Registry**. i.e. `FROM gcr.io/PROJECT_ID/mlops-dev:TF115-TFX015-KFP136` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting base_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {BASE_IMAGE_FOLDER}/Dockerfile\n",
    "FROM gcr.io/demokfp/mlops-dev:TF115-TFX015-KFP136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/demokfp/mlops-dev:TF115-TFX015-KFP136\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build base and trainer images \n",
    "Use **Cloud Build** to build the images and save them to the **Cloud Container Registery**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 49 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://demokfp_cloudbuild/source/1581478226.9-cc40f832117d42449c6931573d4b1789.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/demokfp/builds/43d92e1d-b6af-41fd-9156-6917e2ef850f].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/43d92e1d-b6af-41fd-9156-6917e2ef850f?project=435903989237].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"43d92e1d-b6af-41fd-9156-6917e2ef850f\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://demokfp_cloudbuild/source/1581478226.9-cc40f832117d42449c6931573d4b1789.tgz#1581478227104329\n",
      "Copying gs://demokfp_cloudbuild/source/1581478226.9-cc40f832117d42449c6931573d4b1789.tgz#1581478227104329...\n",
      "/ [1 files][  176.0 B/  176.0 B]                                                \n",
      "Operation completed over 1 objects/176.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/1 : FROM gcr.io/demokfp/mlops-dev:TF115-TFX015-KFP136\n",
      "TF115-TFX015-KFP136: Pulling from demokfp/mlops-dev\n",
      "35c102085707: Pulling fs layer\n",
      "251f5509d51d: Pulling fs layer\n",
      "8e829fe70a46: Pulling fs layer\n",
      "6001e1789921: Pulling fs layer\n",
      "1259902c87a2: Pulling fs layer\n",
      "83ca0edf82af: Pulling fs layer\n",
      "a459cc7a0819: Pulling fs layer\n",
      "7de7778cb300: Pulling fs layer\n",
      "62e0a31a8af6: Pulling fs layer\n",
      "a7785d29f5ab: Pulling fs layer\n",
      "6b76a06da4d7: Pulling fs layer\n",
      "413905cedc93: Pulling fs layer\n",
      "a5d245cced6f: Pulling fs layer\n",
      "8c6be6aa5553: Pulling fs layer\n",
      "1d7154118978: Pulling fs layer\n",
      "1df8626a77b0: Pulling fs layer\n",
      "a44e745885de: Pulling fs layer\n",
      "42e2291ef541: Pulling fs layer\n",
      "dd41a5143d21: Pulling fs layer\n",
      "0e9699f7bcd3: Pulling fs layer\n",
      "747ebef4a5c6: Pulling fs layer\n",
      "7b2fb06f1e43: Pulling fs layer\n",
      "5e2305f6636b: Pulling fs layer\n",
      "6001e1789921: Waiting\n",
      "1259902c87a2: Waiting\n",
      "83ca0edf82af: Waiting\n",
      "a459cc7a0819: Waiting\n",
      "7de7778cb300: Waiting\n",
      "62e0a31a8af6: Waiting\n",
      "a7785d29f5ab: Waiting\n",
      "6b76a06da4d7: Waiting\n",
      "413905cedc93: Waiting\n",
      "a5d245cced6f: Waiting\n",
      "8c6be6aa5553: Waiting\n",
      "1d7154118978: Waiting\n",
      "1df8626a77b0: Waiting\n",
      "a44e745885de: Waiting\n",
      "42e2291ef541: Waiting\n",
      "dd41a5143d21: Waiting\n",
      "0e9699f7bcd3: Waiting\n",
      "747ebef4a5c6: Waiting\n",
      "7b2fb06f1e43: Waiting\n",
      "5e2305f6636b: Waiting\n",
      "8e829fe70a46: Verifying Checksum\n",
      "8e829fe70a46: Download complete\n",
      "251f5509d51d: Verifying Checksum\n",
      "251f5509d51d: Download complete\n",
      "6001e1789921: Verifying Checksum\n",
      "6001e1789921: Download complete\n",
      "35c102085707: Verifying Checksum\n",
      "35c102085707: Download complete\n",
      "83ca0edf82af: Verifying Checksum\n",
      "83ca0edf82af: Download complete\n",
      "a459cc7a0819: Verifying Checksum\n",
      "a459cc7a0819: Download complete\n",
      "62e0a31a8af6: Verifying Checksum\n",
      "62e0a31a8af6: Download complete\n",
      "a7785d29f5ab: Verifying Checksum\n",
      "a7785d29f5ab: Download complete\n",
      "6b76a06da4d7: Verifying Checksum\n",
      "6b76a06da4d7: Download complete\n",
      "413905cedc93: Verifying Checksum\n",
      "413905cedc93: Download complete\n",
      "a5d245cced6f: Verifying Checksum\n",
      "a5d245cced6f: Download complete\n",
      "8c6be6aa5553: Verifying Checksum\n",
      "8c6be6aa5553: Download complete\n",
      "1d7154118978: Verifying Checksum\n",
      "1d7154118978: Download complete\n",
      "1259902c87a2: Verifying Checksum\n",
      "1259902c87a2: Download complete\n",
      "1df8626a77b0: Verifying Checksum\n",
      "1df8626a77b0: Download complete\n",
      "a44e745885de: Verifying Checksum\n",
      "a44e745885de: Download complete\n",
      "dd41a5143d21: Verifying Checksum\n",
      "dd41a5143d21: Download complete\n",
      "0e9699f7bcd3: Verifying Checksum\n",
      "0e9699f7bcd3: Download complete\n",
      "747ebef4a5c6: Verifying Checksum\n",
      "747ebef4a5c6: Download complete\n",
      "42e2291ef541: Verifying Checksum\n",
      "42e2291ef541: Download complete\n",
      "7b2fb06f1e43: Verifying Checksum\n",
      "7b2fb06f1e43: Download complete\n",
      "5e2305f6636b: Verifying Checksum\n",
      "5e2305f6636b: Download complete\n",
      "35c102085707: Pull complete\n",
      "7de7778cb300: Verifying Checksum\n",
      "7de7778cb300: Download complete\n",
      "251f5509d51d: Pull complete\n",
      "8e829fe70a46: Pull complete\n",
      "6001e1789921: Pull complete\n",
      "1259902c87a2: Pull complete\n",
      "83ca0edf82af: Pull complete\n",
      "a459cc7a0819: Pull complete\n",
      "7de7778cb300: Pull complete\n",
      "62e0a31a8af6: Pull complete\n",
      "a7785d29f5ab: Pull complete\n",
      "6b76a06da4d7: Pull complete\n",
      "413905cedc93: Pull complete\n",
      "a5d245cced6f: Pull complete\n",
      "8c6be6aa5553: Pull complete\n",
      "1d7154118978: Pull complete\n",
      "1df8626a77b0: Pull complete\n",
      "a44e745885de: Pull complete\n",
      "42e2291ef541: Pull complete\n",
      "dd41a5143d21: Pull complete\n",
      "0e9699f7bcd3: Pull complete\n",
      "747ebef4a5c6: Pull complete\n",
      "7b2fb06f1e43: Pull complete\n",
      "5e2305f6636b: Pull complete\n",
      "Digest: sha256:e03cd8ebef4a6b1b4ec87b08dc4302dd05e56a0b11a08eb70aefb910439f3542\n",
      "Status: Downloaded newer image for gcr.io/demokfp/mlops-dev:TF115-TFX015-KFP136\n",
      " ---> 22265dc092f5\n",
      "Successfully built 22265dc092f5\n",
      "Successfully tagged gcr.io/demokfp/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/demokfp/base_image:latest\n",
      "The push refers to repository [gcr.io/demokfp/base_image]\n",
      "6006a341d3f7: Preparing\n",
      "e870b96193d3: Preparing\n",
      "43ee21626282: Preparing\n",
      "b875beebc5b1: Preparing\n",
      "3885d9501aa4: Preparing\n",
      "cc4198cb51b8: Preparing\n",
      "98a86238f88d: Preparing\n",
      "07a867e0ba2d: Preparing\n",
      "092c50747c65: Preparing\n",
      "d6fb36f9bda1: Preparing\n",
      "f36c7efe6784: Preparing\n",
      "1f3727b0e386: Preparing\n",
      "80824689ea9a: Preparing\n",
      "dae8971c1728: Preparing\n",
      "63b763e1ea3e: Preparing\n",
      "7d412b9c88ab: Preparing\n",
      "4019db0181d2: Preparing\n",
      "5a78197acff6: Preparing\n",
      "804e87810c15: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "cc4198cb51b8: Waiting\n",
      "98a86238f88d: Waiting\n",
      "07a867e0ba2d: Waiting\n",
      "092c50747c65: Waiting\n",
      "d6fb36f9bda1: Waiting\n",
      "f36c7efe6784: Waiting\n",
      "1f3727b0e386: Waiting\n",
      "80824689ea9a: Waiting\n",
      "dae8971c1728: Waiting\n",
      "63b763e1ea3e: Waiting\n",
      "7d412b9c88ab: Waiting\n",
      "4019db0181d2: Waiting\n",
      "5a78197acff6: Waiting\n",
      "804e87810c15: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "3885d9501aa4: Layer already exists\n",
      "b875beebc5b1: Layer already exists\n",
      "43ee21626282: Layer already exists\n",
      "e870b96193d3: Layer already exists\n",
      "6006a341d3f7: Layer already exists\n",
      "cc4198cb51b8: Layer already exists\n",
      "98a86238f88d: Layer already exists\n",
      "092c50747c65: Layer already exists\n",
      "d6fb36f9bda1: Layer already exists\n",
      "f36c7efe6784: Layer already exists\n",
      "07a867e0ba2d: Layer already exists\n",
      "1f3727b0e386: Layer already exists\n",
      "80824689ea9a: Layer already exists\n",
      "dae8971c1728: Layer already exists\n",
      "63b763e1ea3e: Layer already exists\n",
      "7d412b9c88ab: Layer already exists\n",
      "4019db0181d2: Layer already exists\n",
      "5a78197acff6: Layer already exists\n",
      "804e87810c15: Layer already exists\n",
      "7beb13bce073: Layer already exists\n",
      "122be11ab4a2: Layer already exists\n",
      "6cebf3abed5f: Layer already exists\n",
      "f7eae43028b3: Layer already exists\n",
      "latest: digest: sha256:e03cd8ebef4a6b1b4ec87b08dc4302dd05e56a0b11a08eb70aefb910439f3542 size: 5142\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                            IMAGES                               STATUS\n",
      "43d92e1d-b6af-41fd-9156-6917e2ef850f  2020-02-12T03:30:27+00:00  3M20S     gs://demokfp_cloudbuild/source/1581478226.9-cc40f832117d42449c6931573d4b1789.tgz  gcr.io/demokfp/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "IMAGE_URI_BASE=\"gcr.io/{}/{}:latest\".format(PROJECT_ID,BASE_IMAGE_FOLDER)\n",
    "\n",
    "!gcloud builds submit --timeout 15m --tag {IMAGE_URI_BASE} base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 2.6 KiB before compression.\n",
      "Uploading tarball of [trainer_image] to [gs://demokfp_cloudbuild/source/1581478459.63-166b3a51b65245e281f7907f22e813a6.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/demokfp/builds/e9827cdb-331b-4260-a291-379dd4a05f6d].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/e9827cdb-331b-4260-a291-379dd4a05f6d?project=435903989237].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"e9827cdb-331b-4260-a291-379dd4a05f6d\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://demokfp_cloudbuild/source/1581478459.63-166b3a51b65245e281f7907f22e813a6.tgz#1581478459897769\n",
      "Copying gs://demokfp_cloudbuild/source/1581478459.63-166b3a51b65245e281f7907f22e813a6.tgz#1581478459897769...\n",
      "/ [1 files][  1.2 KiB/  1.2 KiB]                                                \n",
      "Operation completed over 1 objects/1.2 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.656kB\n",
      "Step 1/5 : FROM gcr.io/demokfp/mlops-dev:TF115-TFX015-KFP136\n",
      "TF115-TFX015-KFP136: Pulling from demokfp/mlops-dev\n",
      "35c102085707: Pulling fs layer\n",
      "251f5509d51d: Pulling fs layer\n",
      "8e829fe70a46: Pulling fs layer\n",
      "6001e1789921: Pulling fs layer\n",
      "1259902c87a2: Pulling fs layer\n",
      "83ca0edf82af: Pulling fs layer\n",
      "a459cc7a0819: Pulling fs layer\n",
      "7de7778cb300: Pulling fs layer\n",
      "62e0a31a8af6: Pulling fs layer\n",
      "a7785d29f5ab: Pulling fs layer\n",
      "6b76a06da4d7: Pulling fs layer\n",
      "413905cedc93: Pulling fs layer\n",
      "a5d245cced6f: Pulling fs layer\n",
      "8c6be6aa5553: Pulling fs layer\n",
      "1d7154118978: Pulling fs layer\n",
      "1df8626a77b0: Pulling fs layer\n",
      "a44e745885de: Pulling fs layer\n",
      "42e2291ef541: Pulling fs layer\n",
      "dd41a5143d21: Pulling fs layer\n",
      "0e9699f7bcd3: Pulling fs layer\n",
      "747ebef4a5c6: Pulling fs layer\n",
      "7b2fb06f1e43: Pulling fs layer\n",
      "5e2305f6636b: Pulling fs layer\n",
      "6001e1789921: Waiting\n",
      "1259902c87a2: Waiting\n",
      "83ca0edf82af: Waiting\n",
      "a459cc7a0819: Waiting\n",
      "7de7778cb300: Waiting\n",
      "62e0a31a8af6: Waiting\n",
      "a7785d29f5ab: Waiting\n",
      "6b76a06da4d7: Waiting\n",
      "413905cedc93: Waiting\n",
      "a5d245cced6f: Waiting\n",
      "8c6be6aa5553: Waiting\n",
      "1d7154118978: Waiting\n",
      "1df8626a77b0: Waiting\n",
      "a44e745885de: Waiting\n",
      "42e2291ef541: Waiting\n",
      "dd41a5143d21: Waiting\n",
      "0e9699f7bcd3: Waiting\n",
      "747ebef4a5c6: Waiting\n",
      "7b2fb06f1e43: Waiting\n",
      "5e2305f6636b: Waiting\n",
      "251f5509d51d: Verifying Checksum\n",
      "251f5509d51d: Download complete\n",
      "8e829fe70a46: Verifying Checksum\n",
      "8e829fe70a46: Download complete\n",
      "6001e1789921: Verifying Checksum\n",
      "6001e1789921: Download complete\n",
      "35c102085707: Verifying Checksum\n",
      "35c102085707: Download complete\n",
      "83ca0edf82af: Verifying Checksum\n",
      "83ca0edf82af: Download complete\n",
      "a459cc7a0819: Verifying Checksum\n",
      "a459cc7a0819: Download complete\n",
      "62e0a31a8af6: Verifying Checksum\n",
      "62e0a31a8af6: Download complete\n",
      "a7785d29f5ab: Verifying Checksum\n",
      "a7785d29f5ab: Download complete\n",
      "6b76a06da4d7: Verifying Checksum\n",
      "6b76a06da4d7: Download complete\n",
      "413905cedc93: Verifying Checksum\n",
      "413905cedc93: Download complete\n",
      "1259902c87a2: Verifying Checksum\n",
      "1259902c87a2: Download complete\n",
      "8c6be6aa5553: Verifying Checksum\n",
      "8c6be6aa5553: Download complete\n",
      "a5d245cced6f: Verifying Checksum\n",
      "a5d245cced6f: Download complete\n",
      "1d7154118978: Verifying Checksum\n",
      "1d7154118978: Download complete\n",
      "1df8626a77b0: Verifying Checksum\n",
      "1df8626a77b0: Download complete\n",
      "a44e745885de: Verifying Checksum\n",
      "a44e745885de: Download complete\n",
      "dd41a5143d21: Verifying Checksum\n",
      "dd41a5143d21: Download complete\n",
      "42e2291ef541: Verifying Checksum\n",
      "42e2291ef541: Download complete\n",
      "747ebef4a5c6: Verifying Checksum\n",
      "747ebef4a5c6: Download complete\n",
      "7b2fb06f1e43: Verifying Checksum\n",
      "7b2fb06f1e43: Download complete\n",
      "0e9699f7bcd3: Verifying Checksum\n",
      "0e9699f7bcd3: Download complete\n",
      "5e2305f6636b: Verifying Checksum\n",
      "5e2305f6636b: Download complete\n",
      "35c102085707: Pull complete\n",
      "7de7778cb300: Verifying Checksum\n",
      "7de7778cb300: Download complete\n",
      "251f5509d51d: Pull complete\n",
      "8e829fe70a46: Pull complete\n",
      "6001e1789921: Pull complete\n",
      "1259902c87a2: Pull complete\n",
      "83ca0edf82af: Pull complete\n",
      "a459cc7a0819: Pull complete\n",
      "7de7778cb300: Pull complete\n",
      "62e0a31a8af6: Pull complete\n",
      "a7785d29f5ab: Pull complete\n",
      "6b76a06da4d7: Pull complete\n",
      "413905cedc93: Pull complete\n",
      "a5d245cced6f: Pull complete\n",
      "8c6be6aa5553: Pull complete\n",
      "1d7154118978: Pull complete\n",
      "1df8626a77b0: Pull complete\n",
      "a44e745885de: Pull complete\n",
      "42e2291ef541: Pull complete\n",
      "dd41a5143d21: Pull complete\n",
      "0e9699f7bcd3: Pull complete\n",
      "747ebef4a5c6: Pull complete\n",
      "7b2fb06f1e43: Pull complete\n",
      "5e2305f6636b: Pull complete\n",
      "Digest: sha256:e03cd8ebef4a6b1b4ec87b08dc4302dd05e56a0b11a08eb70aefb910439f3542\n",
      "Status: Downloaded newer image for gcr.io/demokfp/mlops-dev:TF115-TFX015-KFP136\n",
      " ---> 22265dc092f5\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune\n",
      " ---> Running in 98bacc87afc6\n",
      "Collecting fire\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading https://files.pythonhosted.org/packages/84/54/142a00a29d1c51dcf8c93b305f35554c947be2faa0d55de1eabcc0a9023c/cloudml-hypertune-0.1.0.dev6.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: six in /root/miniconda3/lib/python3.5/site-packages (from fire) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor in /root/miniconda3/lib/python3.5/site-packages (from fire) (1.1.0)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103543 sha256=6496b489d49612c0cfe7f25581046620ae69972a83b496b0491d6380be9d9f8f\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3993 sha256=61791b5102fb7b9b86f4af9673d011dc9aede00b7947949b359faad2133d3aac\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/ac/62/80b621f3fe2994f3f367a36123d8351d75e3ea5591b4a62c85\n",
      "Successfully built fire cloudml-hypertune\n",
      "Installing collected packages: fire, cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.2.1\n",
      "\u001b[91mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 98bacc87afc6\n",
      " ---> 67ce0881dd5c\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 46ad7a17a39b\n",
      "Removing intermediate container 46ad7a17a39b\n",
      " ---> 022ce12c8c68\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> ba0de5544949\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in f1a77d4a349b\n",
      "Removing intermediate container f1a77d4a349b\n",
      " ---> 69ca5096c4ba\n",
      "Successfully built 69ca5096c4ba\n",
      "Successfully tagged gcr.io/demokfp/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/demokfp/trainer_image:latest\n",
      "The push refers to repository [gcr.io/demokfp/trainer_image]\n",
      "1e3af9bdefbb: Preparing\n",
      "514a97d11ee1: Preparing\n",
      "9d9a561198a1: Preparing\n",
      "6006a341d3f7: Preparing\n",
      "e870b96193d3: Preparing\n",
      "43ee21626282: Preparing\n",
      "b875beebc5b1: Preparing\n",
      "3885d9501aa4: Preparing\n",
      "cc4198cb51b8: Preparing\n",
      "98a86238f88d: Preparing\n",
      "07a867e0ba2d: Preparing\n",
      "092c50747c65: Preparing\n",
      "d6fb36f9bda1: Preparing\n",
      "f36c7efe6784: Preparing\n",
      "1f3727b0e386: Preparing\n",
      "80824689ea9a: Preparing\n",
      "dae8971c1728: Preparing\n",
      "63b763e1ea3e: Preparing\n",
      "7d412b9c88ab: Preparing\n",
      "4019db0181d2: Preparing\n",
      "5a78197acff6: Preparing\n",
      "804e87810c15: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "43ee21626282: Waiting\n",
      "b875beebc5b1: Waiting\n",
      "3885d9501aa4: Waiting\n",
      "cc4198cb51b8: Waiting\n",
      "98a86238f88d: Waiting\n",
      "07a867e0ba2d: Waiting\n",
      "092c50747c65: Waiting\n",
      "d6fb36f9bda1: Waiting\n",
      "f36c7efe6784: Waiting\n",
      "1f3727b0e386: Waiting\n",
      "80824689ea9a: Waiting\n",
      "dae8971c1728: Waiting\n",
      "63b763e1ea3e: Waiting\n",
      "7d412b9c88ab: Waiting\n",
      "4019db0181d2: Waiting\n",
      "5a78197acff6: Waiting\n",
      "804e87810c15: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "e870b96193d3: Layer already exists\n",
      "6006a341d3f7: Layer already exists\n",
      "b875beebc5b1: Layer already exists\n",
      "43ee21626282: Layer already exists\n",
      "cc4198cb51b8: Layer already exists\n",
      "3885d9501aa4: Layer already exists\n",
      "98a86238f88d: Layer already exists\n",
      "07a867e0ba2d: Layer already exists\n",
      "092c50747c65: Layer already exists\n",
      "d6fb36f9bda1: Layer already exists\n",
      "1f3727b0e386: Layer already exists\n",
      "f36c7efe6784: Layer already exists\n",
      "80824689ea9a: Layer already exists\n",
      "dae8971c1728: Layer already exists\n",
      "63b763e1ea3e: Layer already exists\n",
      "7d412b9c88ab: Layer already exists\n",
      "4019db0181d2: Layer already exists\n",
      "5a78197acff6: Layer already exists\n",
      "804e87810c15: Layer already exists\n",
      "122be11ab4a2: Layer already exists\n",
      "f7eae43028b3: Layer already exists\n",
      "7beb13bce073: Layer already exists\n",
      "6cebf3abed5f: Layer already exists\n",
      "1e3af9bdefbb: Pushed\n",
      "514a97d11ee1: Pushed\n",
      "9d9a561198a1: Pushed\n",
      "latest: digest: sha256:e3c4af341a8e6a1bde8ed30fc3bede086a05cf5fdd832b7273af33e2c9928e79 size: 5767\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                             IMAGES                                  STATUS\n",
      "e9827cdb-331b-4260-a291-379dd4a05f6d  2020-02-12T03:34:20+00:00  3M37S     gs://demokfp_cloudbuild/source/1581478459.63-166b3a51b65245e281f7907f22e813a6.tgz  gcr.io/demokfp/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "IMAGE_URI_TRAIN=\"gcr.io/{}/{}:latest\".format(PROJECT_ID,TRAINING_APP_FOLDER)\n",
    "\n",
    "!gcloud builds submit --timeout 15m --tag {IMAGE_URI_TRAIN} trainer_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Yaml file of the pipeline\n",
    "Now we build the the yaml file for the pipeline based on a file `covertype_training_pipeline.py`. This script outlines the pipeline and the individual components and the appropriate input and outputs. Using the DSL compile command, a static configuration (in YAML format) is created that the Kubeflow Pipelines can execute. \n",
    "\n",
    "**NOTE:** Change the environment settings in `covertype_training_pipeline.py` to reflect your BASE_IMAGE and TRAINER_IMAGE URI's. i.e *'gcr.io/PROJECT_ID/base_image:latest'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py covertype_training_pipeline.py --output covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the pipeline\n",
    "Select a pipeline name, ensure it is not already in use (else a 500 error will be displayed). Deploy the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline ab90cc6c-3f00-480c-bea3-bd1959e7394b has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           ab90cc6c-3f00-480c-bea3-bd1959e7394b\n",
      "Name         covertype_classifier_training1\n",
      "Description\n",
      "Uploaded at  2020-02-12T03:38:20+00:00\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| Parameter Name              | Default Value                                    |\n",
      "+=============================+==================================================+\n",
      "| project_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| region                      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| source_table_name           |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| gcs_root                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_name      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_threshold |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| model_id                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| version_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| replace_existing_version    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| hypertune_settings          | {                                                |\n",
      "|                             |     \"hyperparameters\":  {                        |\n",
      "|                             |         \"goal\": \"MAXIMIZE\",                      |\n",
      "|                             |         \"maxTrials\": 6,                          |\n",
      "|                             |         \"maxParallelTrials\": 3,                  |\n",
      "|                             |         \"hyperparameterMetricTag\": \"accuracy\",   |\n",
      "|                             |         \"enableTrialEarlyStopping\": True,        |\n",
      "|                             |         \"params\": [                              |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"max_iter\",     |\n",
      "|                             |                 \"type\": \"DISCRETE\",              |\n",
      "|                             |                 \"discreteValues\": [500, 1000]    |\n",
      "|                             |             },                                   |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"alpha\",        |\n",
      "|                             |                 \"type\": \"DOUBLE\",                |\n",
      "|                             |                 \"minValue\": 0.0001,              |\n",
      "|                             |                 \"maxValue\": 0.001,               |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\" |\n",
      "|                             |             }                                    |\n",
      "|                             |         ]                                        |\n",
      "|                             |     }                                            |\n",
      "|                             | }                                                |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_location            | US                                               |\n",
      "+-----------------------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='covertype_classifier_training1'\n",
    "\n",
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} pipeline upload -p {PIPELINE_NAME} covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will return a list of pipelines depolyed at the given hostname. We see that `covertype_classifier_training` has been deployed. This list also allows us to copy the pipeline ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} pipeline list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing the pipeline\n",
    "The deployed pipeline can be viewed through the Kubeflow Pipeline UI given at the URL below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://21bce3d410fd3c82-dot-us-central1.notebooks.googleusercontent.com\n"
     ]
    }
   ],
   "source": [
    "print('https://{}'.format(INVERSE_PROXY_HOSTNAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment \n",
    "Now that the pipeline is deployed we want to run an experiment, this will cause the pipeline to run, pulling the data from bigquery and splitting it, training the models, evaluating them and deploy the best performing model. This experiment takes approximately an hour to execute and will result in a deployed model which can be interacted with through GCP's AI platform predicting service. \n",
    "\n",
    "**NOTE:** Change the PIPELINE_ID to reflect the ID copied from above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='ab90cc6c-3f00-480c-bea3-bd1959e7394b'\n",
    "\n",
    "EXPERIMENT_NAME='Covertype_Classifier_Training'\n",
    "RUN_ID='Run_001'\n",
    "SOURCE_TABLE='covertype_dataset.covertype'\n",
    "DATASET_ID='splits'\n",
    "EVALUATION_METRIC='accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD='0.69'\n",
    "MODEL_ID='covertype_classifier'\n",
    "VERSION_ID='v01'\n",
    "REPLACE_EXISTING_VERSION=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 8d0a4358-ea11-4dcf-af1c-582e4ca8f418 is submitted\n",
      "+--------------------------------------+---------+----------+---------------------------+\n",
      "| run id                               | name    | status   | created at                |\n",
      "+======================================+=========+==========+===========================+\n",
      "| 8d0a4358-ea11-4dcf-af1c-582e4ca8f418 | Run_001 |          | 2020-02-12T03:38:55+00:00 |\n",
      "+--------------------------------------+---------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} run submit \\\n",
    "-e Covertype_Classifier_Training \\\n",
    "-r {RUN_ID} \\\n",
    "-p {PIPELINE_ID} \\\n",
    "project_id={PROJECT_ID} \\\n",
    "gcs_root={GCS_STAGING_PATH} \\\n",
    "region={REGION} \\\n",
    "source_table_name={SOURCE_TABLE} \\\n",
    "dataset_id={DATASET_ID} \\\n",
    "evaluation_metric_name={EVALUATION_METRIC} \\\n",
    "evaluation_metric_threshold={EVALUATION_METRIC_THRESHOLD} \\\n",
    "model_id={MODEL_ID} \\\n",
    "version_id={VERSION_ID} \\\n",
    "replace_existing_version={REPLACE_EXISTING_VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
