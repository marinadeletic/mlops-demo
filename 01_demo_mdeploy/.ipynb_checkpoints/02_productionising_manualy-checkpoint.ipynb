{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Productionising the ML model\n",
    "This notebook will walk through the process of creating, building and commiting the artifacts required to run the model developed in the Experimentation Notebook in production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "**NOTE:** Set Project ID to your project  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for mmlops3-cluster.\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'mmlops3'\n",
    "PREFIX = PROJECT_ID\n",
    "REGION = 'us-central1'\n",
    "\n",
    "DATA_ROOT = 'gs://workshop-datasets/covertype'\n",
    "TRAINING_FILE_PATH = DATA_ROOT + '/training/dataset.csv'\n",
    "VALIDATION_FILE_PATH = DATA_ROOT + '/evaluation/dataset.csv'\n",
    "\n",
    "# Job dir for AI Platform Training\n",
    "JOB_DIR_ROOT='gs://{}-artifact-store/jobs'.format(PREFIX)\n",
    "\n",
    "\n",
    "NAMESPACE='kubeflow'\n",
    "ZONE='us-central1-a'\n",
    "ARTIFACT_STORE_URI='gs://{}-artifact-store'.format(PREFIX)\n",
    "GCS_STAGING_PATH='{}/staging'.format(ARTIFACT_STORE_URI)\n",
    "GKE_CLUSTER_NAME='{}-cluster'.format(PREFIX)\n",
    "\n",
    "!gcloud container clusters get-credentials $GKE_CLUSTER_NAME --zone $ZONE\n",
    "HOST_TEMP=!(kubectl describe configmap inverse-proxy-config -n $NAMESPACE | grep \"googleusercontent.com\")\n",
    "INVERSE_PROXY_HOSTNAME=HOST_TEMP[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3ea90122a145b3e7-dot-us-central2.pipelines.googleusercontent.com'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOST_TEMP\n",
    "INVERSE_PROXY_HOSTNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data set to BQ\n",
    "Import the data set from cloud storage to BigQuery. A dataset is created and the table is imported under `covertype_data.covertype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'mmlops3:covertype_dataset' successfully created.\n",
      "Waiting on bqjob_r405ca85a3b467511_00000178a5e48ffd_1 ... (7s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "DATASET_LOCATION='US'\n",
    "DATASET_ID='covertype_dataset'\n",
    "TABLE_ID='covertype'\n",
    "DATA_SOURCE='gs://workshop-datasets/covertype/full/dataset.csv'\n",
    "SCHEMA='Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER'\n",
    "\n",
    "!bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "!bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training application.\n",
    "Now that the data is hosted in BQ, the next step is to create the training application. Start by creating the folders to host the model script, trainer image docker and the base image docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mlops-demo/01_demo_mdeploy\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "#os.chdir('01_demo_mdeploy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'trainer_image'\n",
    "BASE_IMAGE_FOLDER='base_image'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)\n",
    "os.makedirs(BASE_IMAGE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the training script. \n",
    "\n",
    "The script written in the Experimentation Notebook which process the data and trains the classification model is written as a training script `train.py` in the training image folder. In addition to the model written during experimentation an additional `hypertune` function is created which allows for a training job to be run with multiple parameters. This will  run multiple models with a range of parameter - in this case we vary *alpha* and the maximum number of itterations which the model trains for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\"\"\"Covertype Classifier trainer script.\"\"\"\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "  df_train = pd.read_csv(training_dataset_path)\n",
    "  df_validation = pd.read_csv(validation_dataset_path)\n",
    "    \n",
    "  if not hptune:\n",
    "    df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "  numeric_feature_indexes = slice(0, 10)\n",
    "  categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "  preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "  pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "    \n",
    "  num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "  df_train = df_train.astype(num_features_type_map)\n",
    "  df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "  print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "  X_train = df_train.drop('Cover_Type', axis=1)\n",
    "  y_train = df_train['Cover_Type']\n",
    "  \n",
    "  pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  \n",
    "  if hptune:\n",
    "    X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "    y_validation = df_validation['Cover_Type']\n",
    "    accuracy = pipeline.score(X_validation, y_validation)\n",
    "    print('Model accuracy: {}'.format(accuracy))\n",
    "    # Log it with hypertune\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='accuracy',\n",
    "      metric_value=accuracy\n",
    "    )\n",
    "\n",
    "  # Save the model\n",
    "  if not hptune:\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "    print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "The docker images used for the training are based off the image `mlops-dev` created during the inital set up of the environment. Since the AI Platform Notebook instance is based on the `mlops-dev` image we use the same image as a base for the training image. \n",
    "\n",
    "\n",
    "We first write a base image dockerfile which replicates the image used for the Notebook. Then we write a training dockerfile which uses the same base image and add the `train.py` to the image. \n",
    "\n",
    "\n",
    "**NOTE:** Make sure to update the URI for the image so that it points to your project's **Container Registry**. i.e. `FROM gcr.io/PROJECT_ID/mlops-dev:latest` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting base_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {BASE_IMAGE_FOLDER}/Dockerfile\n",
    "FROM gcr.io/mmlops3/mlops-dev:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/mmlops3/mlops-dev:latest\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build base and trainer images \n",
    "Use **Cloud Build** to build the images and save them to the **Cloud Container Registery**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 36 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://mmlops3_cloudbuild/source/1617691081.29-506612261fcc497098c8500bad6ef431.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mmlops3/builds/690ce353-4e64-43ef-bbae-f03a309584df].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/690ce353-4e64-43ef-bbae-f03a309584df?project=286436730533].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"690ce353-4e64-43ef-bbae-f03a309584df\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mmlops3_cloudbuild/source/1617691081.29-506612261fcc497098c8500bad6ef431.tgz#1617691081467474\n",
      "Copying gs://mmlops3_cloudbuild/source/1617691081.29-506612261fcc497098c8500bad6ef431.tgz#1617691081467474...\n",
      "/ [1 files][  160.0 B/  160.0 B]                                                \n",
      "Operation completed over 1 objects/160.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/1 : FROM gcr.io/mmlops3/mlops-dev:latest\n",
      "latest: Pulling from mmlops3/mlops-dev\n",
      "35c102085707: Pulling fs layer\n",
      "251f5509d51d: Pulling fs layer\n",
      "8e829fe70a46: Pulling fs layer\n",
      "6001e1789921: Pulling fs layer\n",
      "1259902c87a2: Pulling fs layer\n",
      "83ca0edf82af: Pulling fs layer\n",
      "a459cc7a0819: Pulling fs layer\n",
      "7de7778cb300: Pulling fs layer\n",
      "62e0a31a8af6: Pulling fs layer\n",
      "a7785d29f5ab: Pulling fs layer\n",
      "6b76a06da4d7: Pulling fs layer\n",
      "413905cedc93: Pulling fs layer\n",
      "a5d245cced6f: Pulling fs layer\n",
      "8c6be6aa5553: Pulling fs layer\n",
      "1d7154118978: Pulling fs layer\n",
      "1df8626a77b0: Pulling fs layer\n",
      "a44e745885de: Pulling fs layer\n",
      "42e2291ef541: Pulling fs layer\n",
      "dd41a5143d21: Pulling fs layer\n",
      "0e9699f7bcd3: Pulling fs layer\n",
      "747ebef4a5c6: Pulling fs layer\n",
      "87b34c56b215: Pulling fs layer\n",
      "7fe7376a458b: Pulling fs layer\n",
      "cf7111184f82: Pulling fs layer\n",
      "83ca0edf82af: Waiting\n",
      "a459cc7a0819: Waiting\n",
      "7de7778cb300: Waiting\n",
      "62e0a31a8af6: Waiting\n",
      "a7785d29f5ab: Waiting\n",
      "6b76a06da4d7: Waiting\n",
      "413905cedc93: Waiting\n",
      "a5d245cced6f: Waiting\n",
      "8c6be6aa5553: Waiting\n",
      "1d7154118978: Waiting\n",
      "1df8626a77b0: Waiting\n",
      "a44e745885de: Waiting\n",
      "42e2291ef541: Waiting\n",
      "dd41a5143d21: Waiting\n",
      "0e9699f7bcd3: Waiting\n",
      "747ebef4a5c6: Waiting\n",
      "87b34c56b215: Waiting\n",
      "7fe7376a458b: Waiting\n",
      "cf7111184f82: Waiting\n",
      "6001e1789921: Waiting\n",
      "1259902c87a2: Waiting\n",
      "8e829fe70a46: Verifying Checksum\n",
      "8e829fe70a46: Download complete\n",
      "251f5509d51d: Verifying Checksum\n",
      "251f5509d51d: Download complete\n",
      "6001e1789921: Verifying Checksum\n",
      "6001e1789921: Download complete\n",
      "35c102085707: Verifying Checksum\n",
      "35c102085707: Download complete\n",
      "83ca0edf82af: Download complete\n",
      "a459cc7a0819: Verifying Checksum\n",
      "a459cc7a0819: Download complete\n",
      "62e0a31a8af6: Verifying Checksum\n",
      "62e0a31a8af6: Download complete\n",
      "a7785d29f5ab: Verifying Checksum\n",
      "a7785d29f5ab: Download complete\n",
      "6b76a06da4d7: Verifying Checksum\n",
      "6b76a06da4d7: Download complete\n",
      "413905cedc93: Verifying Checksum\n",
      "413905cedc93: Download complete\n",
      "a5d245cced6f: Verifying Checksum\n",
      "a5d245cced6f: Download complete\n",
      "8c6be6aa5553: Verifying Checksum\n",
      "8c6be6aa5553: Download complete\n",
      "1d7154118978: Verifying Checksum\n",
      "1d7154118978: Download complete\n",
      "1df8626a77b0: Verifying Checksum\n",
      "1df8626a77b0: Download complete\n",
      "a44e745885de: Verifying Checksum\n",
      "a44e745885de: Download complete\n",
      "1259902c87a2: Verifying Checksum\n",
      "1259902c87a2: Download complete\n",
      "dd41a5143d21: Verifying Checksum\n",
      "dd41a5143d21: Download complete\n",
      "42e2291ef541: Verifying Checksum\n",
      "42e2291ef541: Download complete\n",
      "747ebef4a5c6: Verifying Checksum\n",
      "747ebef4a5c6: Download complete\n",
      "87b34c56b215: Verifying Checksum\n",
      "87b34c56b215: Download complete\n",
      "35c102085707: Pull complete\n",
      "251f5509d51d: Pull complete\n",
      "8e829fe70a46: Pull complete\n",
      "7fe7376a458b: Verifying Checksum\n",
      "7fe7376a458b: Download complete\n",
      "6001e1789921: Pull complete\n",
      "0e9699f7bcd3: Verifying Checksum\n",
      "0e9699f7bcd3: Download complete\n",
      "cf7111184f82: Verifying Checksum\n",
      "cf7111184f82: Download complete\n",
      "7de7778cb300: Verifying Checksum\n",
      "7de7778cb300: Download complete\n",
      "1259902c87a2: Pull complete\n",
      "83ca0edf82af: Pull complete\n",
      "a459cc7a0819: Pull complete\n",
      "7de7778cb300: Pull complete\n",
      "62e0a31a8af6: Pull complete\n",
      "a7785d29f5ab: Pull complete\n",
      "6b76a06da4d7: Pull complete\n",
      "413905cedc93: Pull complete\n",
      "a5d245cced6f: Pull complete\n",
      "8c6be6aa5553: Pull complete\n",
      "1d7154118978: Pull complete\n",
      "1df8626a77b0: Pull complete\n",
      "a44e745885de: Pull complete\n",
      "42e2291ef541: Pull complete\n",
      "dd41a5143d21: Pull complete\n",
      "0e9699f7bcd3: Pull complete\n",
      "747ebef4a5c6: Pull complete\n",
      "87b34c56b215: Pull complete\n",
      "7fe7376a458b: Pull complete\n",
      "cf7111184f82: Pull complete\n",
      "Digest: sha256:2bbd00e73b0949081d9654abf53c9aec4d4fed8e9be0d26b60962c41293054a6\n",
      "Status: Downloaded newer image for gcr.io/mmlops3/mlops-dev:latest\n",
      " ---> d9f958986670\n",
      "Successfully built d9f958986670\n",
      "Successfully tagged gcr.io/mmlops3/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mmlops3/base_image:latest\n",
      "The push refers to repository [gcr.io/mmlops3/base_image]\n",
      "160909312d92: Preparing\n",
      "8d8f5b09c2e1: Preparing\n",
      "197d314a0e10: Preparing\n",
      "43ee21626282: Preparing\n",
      "b875beebc5b1: Preparing\n",
      "3885d9501aa4: Preparing\n",
      "cc4198cb51b8: Preparing\n",
      "98a86238f88d: Preparing\n",
      "07a867e0ba2d: Preparing\n",
      "092c50747c65: Preparing\n",
      "d6fb36f9bda1: Preparing\n",
      "f36c7efe6784: Preparing\n",
      "1f3727b0e386: Preparing\n",
      "80824689ea9a: Preparing\n",
      "dae8971c1728: Preparing\n",
      "63b763e1ea3e: Preparing\n",
      "7d412b9c88ab: Preparing\n",
      "4019db0181d2: Preparing\n",
      "5a78197acff6: Preparing\n",
      "804e87810c15: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "3885d9501aa4: Waiting\n",
      "cc4198cb51b8: Waiting\n",
      "98a86238f88d: Waiting\n",
      "07a867e0ba2d: Waiting\n",
      "092c50747c65: Waiting\n",
      "d6fb36f9bda1: Waiting\n",
      "f36c7efe6784: Waiting\n",
      "1f3727b0e386: Waiting\n",
      "80824689ea9a: Waiting\n",
      "dae8971c1728: Waiting\n",
      "63b763e1ea3e: Waiting\n",
      "7d412b9c88ab: Waiting\n",
      "4019db0181d2: Waiting\n",
      "5a78197acff6: Waiting\n",
      "804e87810c15: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "43ee21626282: Layer already exists\n",
      "b875beebc5b1: Layer already exists\n",
      "197d314a0e10: Layer already exists\n",
      "160909312d92: Layer already exists\n",
      "8d8f5b09c2e1: Layer already exists\n",
      "3885d9501aa4: Layer already exists\n",
      "cc4198cb51b8: Layer already exists\n",
      "98a86238f88d: Layer already exists\n",
      "092c50747c65: Layer already exists\n",
      "07a867e0ba2d: Layer already exists\n",
      "d6fb36f9bda1: Layer already exists\n",
      "f36c7efe6784: Layer already exists\n",
      "1f3727b0e386: Layer already exists\n",
      "80824689ea9a: Layer already exists\n",
      "dae8971c1728: Layer already exists\n",
      "63b763e1ea3e: Layer already exists\n",
      "7d412b9c88ab: Layer already exists\n",
      "5a78197acff6: Layer already exists\n",
      "4019db0181d2: Layer already exists\n",
      "804e87810c15: Layer already exists\n",
      "7beb13bce073: Layer already exists\n",
      "f7eae43028b3: Layer already exists\n",
      "6cebf3abed5f: Layer already exists\n",
      "122be11ab4a2: Layer already exists\n",
      "latest: digest: sha256:2bbd00e73b0949081d9654abf53c9aec4d4fed8e9be0d26b60962c41293054a6 size: 5350\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                             IMAGES                               STATUS\n",
      "690ce353-4e64-43ef-bbae-f03a309584df  2021-04-06T06:38:01+00:00  2M51S     gs://mmlops3_cloudbuild/source/1617691081.29-506612261fcc497098c8500bad6ef431.tgz  gcr.io/mmlops3/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"BASE_IMAGE\"]=\"gcr.io/{}/{}:latest\".format(PROJECT_ID,BASE_IMAGE_FOLDER)\n",
    "BASE_IMAGE=os.environ[\"BASE_IMAGE\"]\n",
    "\n",
    "!gcloud builds submit --timeout 15m --tag {BASE_IMAGE} base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 2.5 KiB before compression.\n",
      "Uploading tarball of [trainer_image] to [gs://mmlops3_cloudbuild/source/1617691303.66-99cc9e9d73d5499fa3e2d6d5bfd7afb6.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mmlops3/builds/d294ddb9-28b1-4f29-9822-f2ca21a2f9fc].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/d294ddb9-28b1-4f29-9822-f2ca21a2f9fc?project=286436730533].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"d294ddb9-28b1-4f29-9822-f2ca21a2f9fc\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mmlops3_cloudbuild/source/1617691303.66-99cc9e9d73d5499fa3e2d6d5bfd7afb6.tgz#1617691303915235\n",
      "Copying gs://mmlops3_cloudbuild/source/1617691303.66-99cc9e9d73d5499fa3e2d6d5bfd7afb6.tgz#1617691303915235...\n",
      "/ [1 files][  1.2 KiB/  1.2 KiB]                                                \n",
      "Operation completed over 1 objects/1.2 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/5 : FROM gcr.io/mmlops3/mlops-dev:latest\n",
      "latest: Pulling from mmlops3/mlops-dev\n",
      "35c102085707: Pulling fs layer\n",
      "251f5509d51d: Pulling fs layer\n",
      "8e829fe70a46: Pulling fs layer\n",
      "6001e1789921: Pulling fs layer\n",
      "1259902c87a2: Pulling fs layer\n",
      "83ca0edf82af: Pulling fs layer\n",
      "a459cc7a0819: Pulling fs layer\n",
      "7de7778cb300: Pulling fs layer\n",
      "62e0a31a8af6: Pulling fs layer\n",
      "a7785d29f5ab: Pulling fs layer\n",
      "6b76a06da4d7: Pulling fs layer\n",
      "413905cedc93: Pulling fs layer\n",
      "a5d245cced6f: Pulling fs layer\n",
      "8c6be6aa5553: Pulling fs layer\n",
      "1d7154118978: Pulling fs layer\n",
      "1df8626a77b0: Pulling fs layer\n",
      "a44e745885de: Pulling fs layer\n",
      "42e2291ef541: Pulling fs layer\n",
      "dd41a5143d21: Pulling fs layer\n",
      "0e9699f7bcd3: Pulling fs layer\n",
      "747ebef4a5c6: Pulling fs layer\n",
      "87b34c56b215: Pulling fs layer\n",
      "7fe7376a458b: Pulling fs layer\n",
      "cf7111184f82: Pulling fs layer\n",
      "1259902c87a2: Waiting\n",
      "83ca0edf82af: Waiting\n",
      "a459cc7a0819: Waiting\n",
      "7de7778cb300: Waiting\n",
      "62e0a31a8af6: Waiting\n",
      "a7785d29f5ab: Waiting\n",
      "6b76a06da4d7: Waiting\n",
      "413905cedc93: Waiting\n",
      "a5d245cced6f: Waiting\n",
      "8c6be6aa5553: Waiting\n",
      "1d7154118978: Waiting\n",
      "1df8626a77b0: Waiting\n",
      "a44e745885de: Waiting\n",
      "42e2291ef541: Waiting\n",
      "dd41a5143d21: Waiting\n",
      "0e9699f7bcd3: Waiting\n",
      "747ebef4a5c6: Waiting\n",
      "87b34c56b215: Waiting\n",
      "7fe7376a458b: Waiting\n",
      "cf7111184f82: Waiting\n",
      "6001e1789921: Waiting\n",
      "8e829fe70a46: Download complete\n",
      "251f5509d51d: Download complete\n",
      "6001e1789921: Download complete\n",
      "35c102085707: Verifying Checksum\n",
      "35c102085707: Download complete\n",
      "a459cc7a0819: Verifying Checksum\n",
      "a459cc7a0819: Download complete\n",
      "83ca0edf82af: Verifying Checksum\n",
      "83ca0edf82af: Download complete\n",
      "62e0a31a8af6: Verifying Checksum\n",
      "62e0a31a8af6: Download complete\n",
      "a7785d29f5ab: Verifying Checksum\n",
      "a7785d29f5ab: Download complete\n",
      "6b76a06da4d7: Verifying Checksum\n",
      "6b76a06da4d7: Download complete\n",
      "413905cedc93: Verifying Checksum\n",
      "413905cedc93: Download complete\n",
      "a5d245cced6f: Verifying Checksum\n",
      "a5d245cced6f: Download complete\n",
      "8c6be6aa5553: Verifying Checksum\n",
      "8c6be6aa5553: Download complete\n",
      "1d7154118978: Verifying Checksum\n",
      "1d7154118978: Download complete\n",
      "1df8626a77b0: Verifying Checksum\n",
      "1df8626a77b0: Download complete\n",
      "a44e745885de: Verifying Checksum\n",
      "a44e745885de: Download complete\n",
      "1259902c87a2: Verifying Checksum\n",
      "1259902c87a2: Download complete\n",
      "35c102085707: Pull complete\n",
      "251f5509d51d: Pull complete\n",
      "dd41a5143d21: Verifying Checksum\n",
      "dd41a5143d21: Download complete\n",
      "8e829fe70a46: Pull complete\n",
      "42e2291ef541: Verifying Checksum\n",
      "42e2291ef541: Download complete\n",
      "747ebef4a5c6: Verifying Checksum\n",
      "747ebef4a5c6: Download complete\n",
      "87b34c56b215: Verifying Checksum\n",
      "87b34c56b215: Download complete\n",
      "6001e1789921: Pull complete\n",
      "7fe7376a458b: Verifying Checksum\n",
      "7fe7376a458b: Download complete\n",
      "0e9699f7bcd3: Verifying Checksum\n",
      "0e9699f7bcd3: Download complete\n",
      "cf7111184f82: Verifying Checksum\n",
      "cf7111184f82: Download complete\n",
      "7de7778cb300: Verifying Checksum\n",
      "7de7778cb300: Download complete\n",
      "1259902c87a2: Pull complete\n",
      "83ca0edf82af: Pull complete\n",
      "a459cc7a0819: Pull complete\n",
      "7de7778cb300: Pull complete\n",
      "62e0a31a8af6: Pull complete\n",
      "a7785d29f5ab: Pull complete\n",
      "6b76a06da4d7: Pull complete\n",
      "413905cedc93: Pull complete\n",
      "a5d245cced6f: Pull complete\n",
      "8c6be6aa5553: Pull complete\n",
      "1d7154118978: Pull complete\n",
      "1df8626a77b0: Pull complete\n",
      "a44e745885de: Pull complete\n",
      "42e2291ef541: Pull complete\n",
      "dd41a5143d21: Pull complete\n",
      "0e9699f7bcd3: Pull complete\n",
      "747ebef4a5c6: Pull complete\n",
      "87b34c56b215: Pull complete\n",
      "7fe7376a458b: Pull complete\n",
      "cf7111184f82: Pull complete\n",
      "Digest: sha256:2bbd00e73b0949081d9654abf53c9aec4d4fed8e9be0d26b60962c41293054a6\n",
      "Status: Downloaded newer image for gcr.io/mmlops3/mlops-dev:latest\n",
      " ---> d9f958986670\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune\n",
      " ---> Running in 1c14818458f1\n",
      "Collecting fire\n",
      "  Downloading https://files.pythonhosted.org/packages/11/07/a119a1aa04d37bc819940d95ed7e135a7dcca1c098123a3764a6dcace9e7/fire-0.4.0.tar.gz (87kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading https://files.pythonhosted.org/packages/84/54/142a00a29d1c51dcf8c93b305f35554c947be2faa0d55de1eabcc0a9023c/cloudml-hypertune-0.1.0.dev6.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: six in /root/miniconda3/lib/python3.5/site-packages (from fire) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor in /root/miniconda3/lib/python3.5/site-packages (from fire) (1.1.0)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=f37531c2279d79e3220d357ce65e518b5320cb92ae5074492a5140d94fccb650\n",
      "  Stored in directory: /root/.cache/pip/wheels/af/19/30/1ea0cad502dcb4e66ed5a690279628c827aea38bbbab75d5ed\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3993 sha256=f2aab5afd50e57a315e5fee9751235696dc18c3b2346f6c646fe6b05a5b874ef\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/ac/62/80b621f3fe2994f3f367a36123d8351d75e3ea5591b4a62c85\n",
      "Successfully built fire cloudml-hypertune\n",
      "Installing collected packages: fire, cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0\n",
      "\u001b[91m/root/miniconda3/lib/python3.5/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.5 support will be dropped in the next release of cryptography. Please upgrade your Python.\n",
      "  from cryptography import x509\n",
      "WARNING: You are using pip version 19.3.1; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 1c14818458f1\n",
      " ---> db9d2d135742\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 2cc3417cf2ab\n",
      "Removing intermediate container 2cc3417cf2ab\n",
      " ---> 5528abf192b2\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> a04fc299d5f2\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in c634771ffc33\n",
      "Removing intermediate container c634771ffc33\n",
      " ---> ff1b5ee79046\n",
      "Successfully built ff1b5ee79046\n",
      "Successfully tagged gcr.io/mmlops3/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mmlops3/trainer_image:latest\n",
      "The push refers to repository [gcr.io/mmlops3/trainer_image]\n",
      "b57fea513a81: Preparing\n",
      "00323f35e3c2: Preparing\n",
      "aee86ae61850: Preparing\n",
      "160909312d92: Preparing\n",
      "8d8f5b09c2e1: Preparing\n",
      "197d314a0e10: Preparing\n",
      "43ee21626282: Preparing\n",
      "b875beebc5b1: Preparing\n",
      "3885d9501aa4: Preparing\n",
      "cc4198cb51b8: Preparing\n",
      "98a86238f88d: Preparing\n",
      "07a867e0ba2d: Preparing\n",
      "092c50747c65: Preparing\n",
      "d6fb36f9bda1: Preparing\n",
      "f36c7efe6784: Preparing\n",
      "1f3727b0e386: Preparing\n",
      "80824689ea9a: Preparing\n",
      "dae8971c1728: Preparing\n",
      "63b763e1ea3e: Preparing\n",
      "7d412b9c88ab: Preparing\n",
      "4019db0181d2: Preparing\n",
      "5a78197acff6: Preparing\n",
      "804e87810c15: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "197d314a0e10: Waiting\n",
      "80824689ea9a: Waiting\n",
      "dae8971c1728: Waiting\n",
      "63b763e1ea3e: Waiting\n",
      "7d412b9c88ab: Waiting\n",
      "4019db0181d2: Waiting\n",
      "5a78197acff6: Waiting\n",
      "804e87810c15: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "43ee21626282: Waiting\n",
      "b875beebc5b1: Waiting\n",
      "3885d9501aa4: Waiting\n",
      "cc4198cb51b8: Waiting\n",
      "98a86238f88d: Waiting\n",
      "07a867e0ba2d: Waiting\n",
      "092c50747c65: Waiting\n",
      "d6fb36f9bda1: Waiting\n",
      "f36c7efe6784: Waiting\n",
      "1f3727b0e386: Waiting\n",
      "160909312d92: Layer already exists\n",
      "8d8f5b09c2e1: Layer already exists\n",
      "197d314a0e10: Layer already exists\n",
      "43ee21626282: Layer already exists\n",
      "3885d9501aa4: Layer already exists\n",
      "b875beebc5b1: Layer already exists\n",
      "cc4198cb51b8: Layer already exists\n",
      "98a86238f88d: Layer already exists\n",
      "07a867e0ba2d: Layer already exists\n",
      "092c50747c65: Layer already exists\n",
      "d6fb36f9bda1: Layer already exists\n",
      "f36c7efe6784: Layer already exists\n",
      "80824689ea9a: Layer already exists\n",
      "1f3727b0e386: Layer already exists\n",
      "dae8971c1728: Layer already exists\n",
      "63b763e1ea3e: Layer already exists\n",
      "7d412b9c88ab: Layer already exists\n",
      "4019db0181d2: Layer already exists\n",
      "5a78197acff6: Layer already exists\n",
      "804e87810c15: Layer already exists\n",
      "122be11ab4a2: Layer already exists\n",
      "7beb13bce073: Layer already exists\n",
      "f7eae43028b3: Layer already exists\n",
      "6cebf3abed5f: Layer already exists\n",
      "b57fea513a81: Pushed\n",
      "00323f35e3c2: Pushed\n",
      "aee86ae61850: Pushed\n",
      "latest: digest: sha256:41ca62e09ee06c3fc0938e16f0c8b75a7d78153215cb15b7e565394ba6458432 size: 5975\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                             IMAGES                                  STATUS\n",
      "d294ddb9-28b1-4f29-9822-f2ca21a2f9fc  2021-04-06T06:41:44+00:00  3M20S     gs://mmlops3_cloudbuild/source/1617691303.66-99cc9e9d73d5499fa3e2d6d5bfd7afb6.tgz  gcr.io/mmlops3/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TRAIN_IMAGE\"]=\"gcr.io/{}/{}:latest\".format(PROJECT_ID,TRAINING_APP_FOLDER)\n",
    "TRAIN_IMAGE=os.environ[\"TRAIN_IMAGE\"]\n",
    "\n",
    "!gcloud builds submit --timeout 15m --tag {TRAIN_IMAGE} trainer_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline YAML\n",
    "Now we build the the yaml file for the pipeline based on a file `covertype_training_pipeline.py`. This script outlines the pipeline and the individual components and the appropriate input and outputs. Using the DSL compile command, a static configuration (in YAML format) is created that the Kubeflow Pipelines can execute. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.5/site-packages/google/auth/crypt/_cryptography_rsa.py:22: CryptographyDeprecationWarning: Python 3.5 support will be dropped in the next release of cryptography. Please upgrade your Python.\n",
      "  import cryptography.exceptions\n"
     ]
    }
   ],
   "source": [
    "!dsl-compile --py covertype_training_pipeline.py --output covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the pipeline\n",
    "Select a pipeline name, ensure it is not already in use at the allocated hostname (else a 500 error will be displayed). Deploy the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.5/site-packages/google/auth/crypt/_cryptography_rsa.py:22: CryptographyDeprecationWarning: Python 3.5 support will be dropped in the next release of cryptography. Please upgrade your Python.\n",
      "  import cryptography.exceptions\n",
      "Pipeline 8d2f1468-7cfa-49bf-bf1b-11728e7970e6 has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           8d2f1468-7cfa-49bf-bf1b-11728e7970e6\n",
      "Name         covertype_classifier_training\n",
      "Description\n",
      "Uploaded at  2021-04-06T06:45:31+00:00\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| Parameter Name              | Default Value                                    |\n",
      "+=============================+==================================================+\n",
      "| project_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| region                      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| source_table_name           |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| gcs_root                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_name      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_threshold |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| model_id                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| version_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| replace_existing_version    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| hypertune_settings          | {                                                |\n",
      "|                             |     \"hyperparameters\":  {                        |\n",
      "|                             |         \"goal\": \"MAXIMIZE\",                      |\n",
      "|                             |         \"maxTrials\": 6,                          |\n",
      "|                             |         \"maxParallelTrials\": 3,                  |\n",
      "|                             |         \"hyperparameterMetricTag\": \"accuracy\",   |\n",
      "|                             |         \"enableTrialEarlyStopping\": True,        |\n",
      "|                             |         \"params\": [                              |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"max_iter\",     |\n",
      "|                             |                 \"type\": \"DISCRETE\",              |\n",
      "|                             |                 \"discreteValues\": [500, 1000]    |\n",
      "|                             |             },                                   |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"alpha\",        |\n",
      "|                             |                 \"type\": \"DOUBLE\",                |\n",
      "|                             |                 \"minValue\": 0.0001,              |\n",
      "|                             |                 \"maxValue\": 0.001,               |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\" |\n",
      "|                             |             }                                    |\n",
      "|                             |         ]                                        |\n",
      "|                             |     }                                            |\n",
      "|                             | }                                                |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_location            | US                                               |\n",
      "+-----------------------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='covertype_classifier_training'\n",
    "\n",
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} pipeline upload -p {PIPELINE_NAME} covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will return a list of pipelines depolyed at the given hostname. We see that `covertype_classifier_training` has been deployed. This list also allows us to copy the pipeline ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.5/site-packages/google/auth/crypt/_cryptography_rsa.py:22: CryptographyDeprecationWarning: Python 3.5 support will be dropped in the next release of cryptography. Please upgrade your Python.\n",
      "  import cryptography.exceptions\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                            | Uploaded at               |\n",
      "+======================================+=================================================+===========================+\n",
      "| 8d2f1468-7cfa-49bf-bf1b-11728e7970e6 | covertype_classifier_training                   | 2021-04-06T06:45:31+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 06e86792-dce6-4ff9-bf9e-0409a245792c | [Tutorial] DSL - Control structures             | 2021-04-06T06:24:19+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| da511c20-4357-413b-890e-dd6533b2bce0 | [Tutorial] Data passing in python components    | 2021-04-06T06:24:18+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 5e9345d6-3d49-4290-af87-1d1c04d32b4a | [Demo] TFX - Iris classification pipeline       | 2021-04-06T06:24:17+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 5ed1131f-e9b2-4e05-88e2-8280c7562c9b | [Demo] TFX - Taxi tip prediction model trainer  | 2021-04-06T06:24:16+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 6d943efe-051c-4aae-b1a3-779299abd925 | [Demo] XGBoost - Training with confusion matrix | 2021-04-06T06:24:15+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} pipeline list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing the pipeline\n",
    "The deployed pipeline can be viewed through the Kubeflow Pipeline UI given at the URL below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://3ea90122a145b3e7-dot-us-central2.pipelines.googleusercontent.com\n"
     ]
    }
   ],
   "source": [
    "print('https://{}'.format(INVERSE_PROXY_HOSTNAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment \n",
    "Now that the pipeline is deployed we want to run an experiment, this will cause the pipeline to run, pulling the data from bigquery and splitting it, training the models, evaluating them and deploy the best performing model. This experiment takes approximately an hour to execute and will result in a deployed model which can be interacted with through GCP's AI platform predicting service. \n",
    "\n",
    "**NOTE:** Change the PIPELINE_ID to reflect the ID copied from above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='8d2f1468-7cfa-49bf-bf1b-11728e7970e6'\n",
    "\n",
    "EXPERIMENT_NAME='Covertype_Classifier_Training'\n",
    "RUN_ID='Run_001'\n",
    "SOURCE_TABLE='covertype_dataset.covertype'\n",
    "DATASET_ID='splits'\n",
    "EVALUATION_METRIC='accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD='0.69'\n",
    "MODEL_ID='covertype_classifier'\n",
    "VERSION_ID='v01'\n",
    "REPLACE_EXISTING_VERSION=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.5/site-packages/google/auth/crypt/_cryptography_rsa.py:22: CryptographyDeprecationWarning: Python 3.5 support will be dropped in the next release of cryptography. Please upgrade your Python.\n",
      "  import cryptography.exceptions\n",
      "Creating experiment Covertype_Classifier_Training.\n",
      "Run 0d2e06e6-4a11-4c92-b34c-668181c53e1c is submitted\n",
      "+--------------------------------------+---------+----------+---------------------------+\n",
      "| run id                               | name    | status   | created at                |\n",
      "+======================================+=========+==========+===========================+\n",
      "| 0d2e06e6-4a11-4c92-b34c-668181c53e1c | Run_001 |          | 2021-04-06T06:47:44+00:00 |\n",
      "+--------------------------------------+---------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} run submit \\\n",
    "-e Covertype_Classifier_Training \\\n",
    "-r {RUN_ID} \\\n",
    "-p {PIPELINE_ID} \\\n",
    "project_id={PROJECT_ID} \\\n",
    "gcs_root={GCS_STAGING_PATH} \\\n",
    "region={REGION} \\\n",
    "source_table_name={SOURCE_TABLE} \\\n",
    "dataset_id={DATASET_ID} \\\n",
    "evaluation_metric_name={EVALUATION_METRIC} \\\n",
    "evaluation_metric_threshold={EVALUATION_METRIC_THRESHOLD} \\\n",
    "model_id={MODEL_ID} \\\n",
    "version_id={VERSION_ID} \\\n",
    "replace_existing_version={REPLACE_EXISTING_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model\n",
    "To test the model we can use the AI platforms prediction API to ask for a prediction based on a JSON input aternatively we can use the prediction UI and input: *{\"instances\":[[2395,0,0,60,6,1170,218,238,156,1054,\"Cache\",\"C2717\"]]}* in the test case window.\n",
    "\n",
    "We write a prediction JSON file with a set of data points, the correct cover types are 6 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predict.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict.json\n",
    "[3366,122,15,789,127,2881,244,227,107,2437,\"Commanche\",\"C8772\"]\n",
    "[2791,340,15,30,10,3906,188,217,168,5401,\"Rawah\",\"C7745\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 1]\n"
     ]
    }
   ],
   "source": [
    "INPUT_DATA_FILE=\"./predict.json\"\n",
    "\n",
    "!gcloud ai-platform predict --model {MODEL_ID} \\\n",
    "  --version {VERSION_ID} \\\n",
    "  --json-instances {INPUT_DATA_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Clean up demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r predict.json covertype_training_pipeline.yaml trainer_image base_image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
