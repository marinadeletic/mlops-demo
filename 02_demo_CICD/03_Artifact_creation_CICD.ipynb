{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Productionising the ML model\n",
    "This notebook will walk through the process of creating, building and commiting the artifacts required to run the model developed in the Experimentation Notebook in production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "**NOTE:** Set Project ID to your project  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'marinas-demo'\n",
    "PREFIX = PROJECT_ID\n",
    "REGION = 'us-central1'\n",
    "\n",
    "DATA_ROOT = 'gs://workshop-datasets/covertype'\n",
    "TRAINING_FILE_PATH = DATA_ROOT + '/training/dataset.csv'\n",
    "VALIDATION_FILE_PATH = DATA_ROOT + '/evaluation/dataset.csv'\n",
    "\n",
    "# Job dir for AI Platform Training\n",
    "JOB_DIR_ROOT='gs://{}-artifact-store/jobs'.format(PREFIX)\n",
    "\n",
    "\n",
    "NAMESPACE='kubeflow'\n",
    "ZONE='us-central1-a'\n",
    "ARTIFACT_STORE_URI='gs://{}-artifact-store'.format(PREFIX)\n",
    "GCS_STAGING_PATH='{}/staging'.format(ARTIFACT_STORE_URI)\n",
    "GKE_CLUSTER_NAME='{}-cluster'.format(PREFIX)\n",
    "\n",
    "!gcloud container clusters get-credentials $GKE_CLUSTER_NAME --zone $ZONE\n",
    "HOST_TEMP=!(kubectl describe configmap inverse-proxy-config -n $NAMESPACE | grep \"googleusercontent.com\")\n",
    "INVERSE_PROXY_HOSTNAME=HOST_TEMP[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data set to BQ\n",
    "Import the data set from cloud storage to BigQuery. A dataset is created and the table is imported under `covertype_data.covertype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LOCATION='US'\n",
    "DATASET_ID='covertype_dataset'\n",
    "TABLE_ID='covertype'\n",
    "DATA_SOURCE='gs://workshop-datasets/covertype/full/dataset.csv'\n",
    "SCHEMA='Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER'\n",
    "\n",
    "!bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "!bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training application.\n",
    "Now that the data is hosted in BQ and we have created the KFP CLI Builder image, the next step is to create the training application. Start by creating the master pipeline folder and nested folders to host the model script, trainer image docker and the base image docker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "#os.chdir('../02_demo_CICD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_APP_FOLDER ='pipeline'\n",
    "TRAINING_APP_FOLDER = 'pipeline/trainer_image'\n",
    "BASE_IMAGE_FOLDER='pipeline/base_image'\n",
    "os.makedirs(PIPELINE_APP_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)\n",
    "os.makedirs(BASE_IMAGE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the training script. \n",
    "\n",
    "The script written in the Experimentation Notebook, which process the data and trains the classification model, is written as a training script `train.py` in the training image folder. In addition to the model written during experimentation, an additional `hypertune` function is created which allows for a training job to be run with multiple parameters. This will  run multiple models with a range of parameters on CAIP training platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\"\"\"Covertype Classifier trainer script.\"\"\"\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "  df_train = pd.read_csv(training_dataset_path)\n",
    "  df_validation = pd.read_csv(validation_dataset_path)\n",
    "    \n",
    "  if not hptune:\n",
    "    df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "  numeric_feature_indexes = slice(0, 10)\n",
    "  categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "  preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "  pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "    \n",
    "  num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "  df_train = df_train.astype(num_features_type_map)\n",
    "  df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "  print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "  X_train = df_train.drop('Cover_Type', axis=1)\n",
    "  y_train = df_train['Cover_Type']\n",
    "  \n",
    "  pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  \n",
    "  if hptune:\n",
    "    X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "    y_validation = df_validation['Cover_Type']\n",
    "    accuracy = pipeline.score(X_validation, y_validation)\n",
    "    print('Model accuracy: {}'.format(accuracy))\n",
    "    # Log it with hypertune\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='accuracy',\n",
    "      metric_value=accuracy\n",
    "    )\n",
    "\n",
    "  # Save the model\n",
    "  if not hptune:\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "    print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write pipeline script and accompanying helper components \n",
    "Now that we have images with the base dependancies and the training script built and saved into the container registry, the next step is to write the pipeline script `covertype_training_pipeline.py` which defines the pipeline which will be deployed to KFP. This file sets hypertuning settings and uses both pre-built components and custome built components defined in a seperate `helper_components.py`.  \n",
    "\n",
    "\n",
    "- Pre-build components. The pipeline uses the following pre-build components that are included with KFP distribution:\n",
    "    - BigQuery query component\n",
    "    - AI Platform Training component\n",
    "    - AI Platform Deploy component\n",
    "- Custom components. The pipeline uses two custom helper components that encapsulate functionality not available in any of the pre-build components. The components are implemented using the KFP SDK's Lightweight Python Components mechanism. The code for the components is in the helper_components.py file:\n",
    "    - Retrieve Best Run- This component retrieves the tuning metric and hyperparameter values for the best run of the AI Platform Training hyperparameter tuning job.\n",
    "    - Evaluate Model - This component evaluates the sklearn trained model using a provided metric and a testing dataset.\n",
    "\n",
    "The workflow implemented by the pipeline is defined using a Python based KFP Domain Specific Language (DSL). The pipeline's DSL is in the covertype_training_pipeline.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PIPELINE_APP_FOLDER}/covertype_training_pipeline.py\n",
    "\n",
    "\"\"\"KFP pipeline orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "\n",
    "\n",
    "TRAINING_FILE_PATH = 'datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 6,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "  \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "  sampling_query_template = \"\"\"\n",
    "       SELECT *\n",
    "       FROM \n",
    "           `{{ source_table }}` AS cover\n",
    "       WHERE \n",
    "       MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "       \"\"\"\n",
    "  query = Template(sampling_query_template).render(\n",
    "      source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "  return query\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_train(project_id: GCPProjectID,\n",
    "                    region: GCPRegion,\n",
    "                    source_table_name: String,\n",
    "                    gcs_root: GCSPath,\n",
    "                    dataset_id: str,\n",
    "                    evaluation_metric_name: str,\n",
    "                    evaluation_metric_threshold: float,\n",
    "                    model_id: str,\n",
    "                    version_id: str,\n",
    "                    replace_existing_version: bool,\n",
    "                    hypertune_settings: Dict = HYPERTUNE_SETTINGS,\n",
    "                    dataset_location: str = 'US'):\n",
    "  \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "  # Create the training split\n",
    "  query = generate_sampling_query(\n",
    "      source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "  training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "  create_training_split = bigquery_query_op(\n",
    "      query=query,\n",
    "      project_id=project_id,\n",
    "      dataset_id=dataset_id,\n",
    "      table_id='',\n",
    "      output_gcs_path=training_file_path,\n",
    "      dataset_location=dataset_location)\n",
    "\n",
    "  # Create the validation split\n",
    "  query = generate_sampling_query(\n",
    "      source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "  validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "  create_validation_split = bigquery_query_op(\n",
    "      query=query,\n",
    "      project_id=project_id,\n",
    "      dataset_id=dataset_id,\n",
    "      table_id='',\n",
    "      output_gcs_path=validation_file_path,\n",
    "      dataset_location=dataset_location)\n",
    "\n",
    "  # Create the testing split\n",
    "  query = generate_sampling_query(\n",
    "      source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "  testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "  create_testing_split = bigquery_query_op(\n",
    "      query=query,\n",
    "      project_id=project_id,\n",
    "      dataset_id=dataset_id,\n",
    "      table_id='',\n",
    "      output_gcs_path=testing_file_path,\n",
    "      dataset_location=dataset_location)\n",
    "\n",
    "  # Tune hyperparameters\n",
    "  tune_args = [\n",
    "      '--training_dataset_path',\n",
    "      create_training_split.outputs['output_gcs_path'],\n",
    "      '--validation_dataset_path',\n",
    "      create_validation_split.outputs['output_gcs_path'], '--hptune', 'True'\n",
    "  ]\n",
    "\n",
    "  job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                              kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "  hypertune = mlengine_train_op(\n",
    "      project_id=project_id,\n",
    "      region=region,\n",
    "      master_image_uri=TRAINER_IMAGE,\n",
    "      job_dir=job_dir,\n",
    "      args=tune_args,\n",
    "      training_input=hypertune_settings)\n",
    "\n",
    "  # Retrieve the best trial\n",
    "  get_best_trial = retrieve_best_run_op(project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "  # Train the model on a combined training and validation datasets\n",
    "  job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "  train_args = [\n",
    "      '--training_dataset_path',\n",
    "      create_training_split.outputs['output_gcs_path'],\n",
    "      '--validation_dataset_path',\n",
    "      create_validation_split.outputs['output_gcs_path'], '--alpha',\n",
    "      get_best_trial.outputs['alpha'], '--max_iter',\n",
    "      get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "  ]\n",
    "\n",
    "  train_model = mlengine_train_op(\n",
    "      project_id=project_id,\n",
    "      region=region,\n",
    "      master_image_uri=TRAINER_IMAGE,\n",
    "      job_dir=job_dir,\n",
    "      args=train_args)\n",
    "\n",
    "  # Evaluate the model on the testing split\n",
    "  eval_model = evaluate_model_op(\n",
    "      dataset_path=str(create_testing_split.outputs['output_gcs_path']),\n",
    "      model_path=str(train_model.outputs['job_dir']),\n",
    "      metric_name=evaluation_metric_name)\n",
    "\n",
    "  # Deploy the model if the primary metric is better than threshold\n",
    "  with kfp.dsl.Condition(\n",
    "      eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "    deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "  kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the custom 'helper' components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PIPELINE_APP_FOLDER}/helper_components.py\n",
    "\n",
    "\"\"\"Helper components.\"\"\"\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(\n",
    "    project_id: str, job_id: str\n",
    ") -> NamedTuple('Outputs', [('metric_value', float), ('alpha', float),\n",
    "                            ('max_iter', int)]):\n",
    "  \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "  from googleapiclient import discovery\n",
    "  from googleapiclient import errors\n",
    "\n",
    "  ml = discovery.build('ml', 'v1')\n",
    "\n",
    "  job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "  request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "  try:\n",
    "    response = request.execute()\n",
    "  except errors.HttpError as err:\n",
    "    print(err)\n",
    "  except:\n",
    "    print('Unexpected error')\n",
    "\n",
    "  print(response)\n",
    "\n",
    "  best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "  metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "  alpha = float(best_trial['hyperparameters']['alpha'])\n",
    "  max_iter = int(best_trial['hyperparameters']['max_iter'])\n",
    "\n",
    "  return (metric_value, alpha, max_iter)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    dataset_path: str, model_path: str, metric_name: str\n",
    ") -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "  \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "  #import joblib\n",
    "  import pickle\n",
    "  import json\n",
    "  import pandas as pd\n",
    "  import subprocess\n",
    "  import sys\n",
    "\n",
    "  from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "  df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "  X_test = df_test.drop('Cover_Type', axis=1)\n",
    "  y_test = df_test['Cover_Type']\n",
    "\n",
    "  # Copy the model from GCS\n",
    "  model_filename = 'model.pkl'\n",
    "  gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "  print(gcs_model_filepath)\n",
    "  subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n",
    "                        stderr=sys.stdout)\n",
    "\n",
    "  with open(model_filename, 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "  y_hat = model.predict(X_test)\n",
    "\n",
    "  if metric_name == 'accuracy':\n",
    "    metric_value = accuracy_score(y_test, y_hat)\n",
    "  elif metric_name == 'recall':\n",
    "    metric_value = recall_score(y_test, y_hat)\n",
    "  else:\n",
    "    metric_name = 'N/A'\n",
    "    metric_value = 0\n",
    "\n",
    "  # Export the metric\n",
    "  metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "  }\n",
    "\n",
    "  return (metric_name, metric_value, json.dumps(metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating KFP CLI builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile \n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install https://storage.googleapis.com/ml-pipeline/release/0.1.36/kfp.tar.gz \n",
    "\n",
    "ENTRYPOINT [\"/bin/bash\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='kfp-cli'\n",
    "IMAGE_URI=\"gcr.io/{}/{}:latest\".format(PROJECT_ID,IMAGE_NAME)\n",
    "!gcloud builds submit --timeout 15m --tag {IMAGE_URI} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "The docker images used for the training are based off the image `mlops-dev:latest` created during the inital set up of the environment. Since the AI Platform Notebook instance is based on the `mlops-dev:latest` image we use the same image as a base for the training image. \n",
    "\n",
    "\n",
    "We first write a base image dockerfile which replicates the image used for the Notebook. Then we write a training dockerfile which uses the same base image and add the `train.py` to the image. \n",
    "\n",
    "\n",
    "**NOTE:** Make sure to update the URI for the image so that it points to your project's **Container Registry**. i.e. `FROM gcr.io/PROJECT_ID/kfp-cli:latest` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/marinas-demo/mlops-dev:latest\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {BASE_IMAGE_FOLDER}/Dockerfile\n",
    "FROM gcr.io/marinas-demo/mlops-dev:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cloud build config file\n",
    "This file is used by **Cloud Build** to create all container images required and deploys the pipeline to kubeflow pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "# Build the trainer image\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/trainer_image\n",
    "  \n",
    "# Build the base image for lightweight components\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/base_image\n",
    "\n",
    "# Compile the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    dsl-compile --py $_PIPELINE_DSL --output $_PIPELINE_PACKAGE\n",
    "  env:\n",
    "  - 'BASE_IMAGE=gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'TRAINER_IMAGE=gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'RUNTIME_VERSION=$_RUNTIME_VERSION'\n",
    "  - 'PYTHON_VERSION=$_PYTHON_VERSION'\n",
    "  - 'COMPONENT_URL_SEARCH_PREFIX=$_COMPONENT_URL_SEARCH_PREFIX'\n",
    "  dir: $_PIPELINE_FOLDER\n",
    "  \n",
    " # Upload the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    kfp --endpoint $_INVERTING_PROXY_HOST pipeline upload -p ${_PIPELINE_NAME}_$TAG_NAME $_PIPELINE_PACKAGE\n",
    "  dir: $_PIPELINE_FOLDER\n",
    "\n",
    "\n",
    "# Push the images to Container Registry \n",
    "images: ['gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', 'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit to Cloud Source Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Source Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME='{}-test'.format(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open terminal in Jupyter notebook and set up an authentication channel to cloud source repo. Run the gcloud init command and follow prompts, create a new source repo with the name of your project ID  \n",
    "`gcloud init && git config credential.helper gcloud.sh` - select option 2, then yes, follow the link, copy the verification code back in the terminal command. Once verified, select [1] Enter project ID and type your project ID, then select no for configure region.\n",
    "\n",
    "`gcloud source repos create REPO_NAME` - you will need to specify your REPO_NAME \n",
    "\n",
    "*This step is simpler if done in terminal*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/mlops-demo\n",
    "!git remote add {REPO_NAME} https://source.developers.google.com/p/{PROJECT_ID}/r/{REPO_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 01a9017] Upload ML\n",
      " 2 files changed, 91 insertions(+), 31 deletions(-)\n",
      " delete mode 100644 02_demo_CICD/trigger_build.yaml\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Upload ML\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting objects: 17, done.\n",
      "Delta compression using up to 4 threads.\n",
      "Compressing objects: 100% (17/17), done.\n",
      "Writing objects: 100% (17/17), 5.21 KiB | 1.74 MiB/s, done.\n",
      "Total 17 (delta 12), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (12/12)\u001b[K\u001b[K\n",
      "To https://source.developers.google.com/p/marinas-demo/r/marinas-demo-test\n",
      "   e0677ec..01a9017  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git push --all {REPO_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cloud Build trigger\n",
    "This will cause the images to build and will deploy the pipeline apon a push to the source repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a trigger build configuration file\n",
    "This file is used by cloud trigger, to pass the variables required by cloud build. \n",
    "\n",
    "**NOTE** Ensure to change the `Project_ID`, `repoName` and `_INVERTING_PROXY_HOST` variables to reflect your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4bcc62bc46b37077-dot-us-central1.notebooks.googleusercontent.com\n"
     ]
    }
   ],
   "source": [
    "print(INVERSE_PROXY_HOSTNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trigger_build.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile trigger_build.yaml\n",
    "\n",
    "description: triggers cloud build upon push\n",
    "name: trigger-build\n",
    "filename: 02_demo_CICD/cloudbuild.yaml\n",
    "triggerTemplate:\n",
    "    projectId: marinas-demo\n",
    "    repoName: marinas-demo-test\n",
    "    branchName: master\n",
    "substitutions:\n",
    "    _INVERTING_PROXY_HOST: 4bcc62bc46b37077-dot-us-central1.notebooks.googleusercontent.com\n",
    "    _TRAINER_IMAGE_NAME: trainer_image\n",
    "    _BASE_IMAGE_NAME: base_image\n",
    "    TAG_NAME: test\n",
    "    _PIPELINE_FOLDER: 02_demo_CICD/pipeline\n",
    "    _PIPELINE_DSL: covertype_training_pipeline.py\n",
    "    _PIPELINE_PACKAGE: covertype_training_pipeline.yaml\n",
    "    _PIPELINE_NAME: covertype_training_deployment\n",
    "    _RUNTIME_VERSION: \"1.14\"\n",
    "    _PYTHON_VERSION: \"3.5\"\n",
    "    _COMPONENT_URL_SEARCH_PREFIX: https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command creates a **Cloud Build** trigger upon any new code committed to the master of our repository. \n",
    "\n",
    "**NOTE**: Only run this command once or it will create multiple triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created [https://cloudbuild.googleapis.com/v1/projects/marinas-demo/triggers/7c9ed5f5-abc7-45dd-8aab-e5f302bbb87b].\n",
      "NAME           CREATE_TIME                STATUS\n",
      "trigger-build  2020-02-18T04:44:13+00:00\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta builds triggers create cloud-source-repositories --trigger-config=trigger_build.yaml --project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run trigger on push\n",
    "We will now push code to our repository, which will automatically trigger a cloud build of the new container images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 41b17e8] Upload ML\n",
      " 2 files changed, 41 insertions(+), 85 deletions(-)\n",
      " create mode 100644 02_demo_CICD/trigger_build.yaml\n",
      "Counting objects: 5, done.\n",
      "Delta compression using up to 4 threads.\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 1.23 KiB | 1.23 MiB/s, done.\n",
      "Total 5 (delta 3), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (3/3)\u001b[K\u001b[K\n",
      "To https://source.developers.google.com/p/marinas-demo/r/marinas-demo-test\n",
      "   01a9017..41b17e8  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"Upload ML\"\n",
    "!git push --all {REPO_NAME}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                    CREATE_TIME                DURATION  SOURCE                    IMAGES  STATUS\n",
      "9b495451-9790-4c7f-8a6a-4e047744bff9  2020-02-18T04:55:41+00:00  3M1S      marinas-demo-test@master  -       WORKING\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds list --ongoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                                     | Uploaded at               |\n",
      "+======================================+==========================================================+===========================+\n",
      "| 1a190a87-8871-4f5a-a7fb-3d760f6fc8b8 | covertype_training_deployment_test                       | 2020-02-18T05:02:01+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| 6780c6f8-d311-437d-8a9a-0c257451fbc0 | [Sample] Basic - Exit Handler                            | 2020-02-17T23:36:34+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| 4473b537-5561-494d-bf68-4e5bf26abff0 | [Sample] Basic - Conditional execution                   | 2020-02-17T23:36:33+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| 36bdbb70-e577-41ca-9bf7-b40475ae8416 | [Sample] Basic - Parallel execution                      | 2020-02-17T23:36:31+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| ec6d8c62-0c5c-4504-a58e-f2a6bab64c69 | [Sample] Basic - Sequential execution                    | 2020-02-17T23:36:30+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| c35f3999-39b6-46fc-b382-d521283263ef | [Sample] Unified DSL - Taxi Tip Prediction Model Trainer | 2020-02-17T23:36:29+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| 826a9106-9cd1-4a5c-99c6-e5c2c3ee5f28 | [Sample] ML - XGBoost - Training with Confusion Matrix   | 2020-02-17T23:36:28+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} pipeline list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the pipeline\n",
    "The deployed pipeline can be viewed through the Kubeflow Pipeline UI given at the URL below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://4bcc62bc46b37077-dot-us-central1.notebooks.googleusercontent.com\n"
     ]
    }
   ],
   "source": [
    "print('https://{}'.format(INVERSE_PROXY_HOSTNAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment \n",
    "Now that the pipeline is deployed we want to run an experiment, this will cause the pipeline to run, pulling the data from bigquery and splitting it, training the models, evaluating them and deploy the best performing model. This experiment takes approximately an hour to execute and will result in a deployed model which can be interacted with through GCP's AI platform predicting service. \n",
    "\n",
    "**NOTE:** Change the PIPELINE_ID to reflect the ID copied from above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='ab90cc6c-3f00-480c-bea3-bd1959e7394b'\n",
    "\n",
    "EXPERIMENT_NAME='Covertype_Classifier_Training'\n",
    "RUN_ID='Run_001'\n",
    "SOURCE_TABLE='covertype_dataset.covertype'\n",
    "DATASET_ID='splits'\n",
    "EVALUATION_METRIC='accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD='0.69'\n",
    "MODEL_ID='covertype_classifier'\n",
    "VERSION_ID='v01'\n",
    "REPLACE_EXISTING_VERSION=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} run submit \\\n",
    "-e Covertype_Classifier_Training \\\n",
    "-r {RUN_ID} \\\n",
    "-p {PIPELINE_ID} \\\n",
    "project_id={PROJECT_ID} \\\n",
    "gcs_root={GCS_STAGING_PATH} \\\n",
    "region={REGION} \\\n",
    "source_table_name={SOURCE_TABLE} \\\n",
    "dataset_id={DATASET_ID} \\\n",
    "evaluation_metric_name={EVALUATION_METRIC} \\\n",
    "evaluation_metric_threshold={EVALUATION_METRIC_THRESHOLD} \\\n",
    "model_id={MODEL_ID} \\\n",
    "version_id={VERSION_ID} \\\n",
    "replace_existing_version={REPLACE_EXISTING_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model\n",
    "To test the model we can use the AI platforms prediction API to ask for a prediction based on a JSON input aternatively we can use the prediction UI and input: *{\"instances\":[[2395,0,0,60,6,1170,218,238,156,1054,\"Cache\",\"C2717\"]]}* in the test case window.\n",
    "\n",
    "We write a prediction JSON file with a set of data points, the correct cover types are 3 and 2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predict.json\n",
    "[2395,0,0,60,6,1170,218,238,156,1054,\"Cache\",\"C2717\"]\n",
    "[2756,135,0,85,14,1608,219,238,156,2451,\"Rawah\",\"C4744\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE=\"/home/mlops-demo/predict.json\"\n",
    "\n",
    "!gcloud ai-platform predict --model $MODEL_ID \\\n",
    "  --version $VERSION_ID \\\n",
    "  --json-instances $INPUT_DATA_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r pipeline/ cloudbuild.yaml build_pipeline.sh Dockerfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
