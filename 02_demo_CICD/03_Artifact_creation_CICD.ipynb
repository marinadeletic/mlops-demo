{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Productionising the ML model\n",
    "This notebook will walk through the process of creating, building and commiting the artifacts required to run the model developed in the Experimentation Notebook in production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "**NOTE:** Set Project ID to your project  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for marina-test-proj-cluster.\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'marina-test-proj'\n",
    "PREFIX = PROJECT_ID\n",
    "REGION = 'us-central1'\n",
    "\n",
    "DATA_ROOT = 'gs://workshop-datasets/covertype/data_validation'\n",
    "TRAINING_FILE_PATH = DATA_ROOT + '/training/dataset.csv'\n",
    "VALIDATION_FILE_PATH = DATA_ROOT + '/evaluation/dataset.csv'\n",
    "\n",
    "# Job dir for AI Platform Training\n",
    "JOB_DIR_ROOT='gs://{}-artifact-store/jobs'.format(PREFIX)\n",
    "\n",
    "\n",
    "NAMESPACE='kubeflow'\n",
    "ZONE='us-central1-a'\n",
    "ARTIFACT_STORE_URI='gs://{}-artifact-store'.format(PREFIX)\n",
    "GCS_STAGING_PATH='{}/staging'.format(ARTIFACT_STORE_URI)\n",
    "GKE_CLUSTER_NAME='{}-cluster'.format(PREFIX)\n",
    "\n",
    "!gcloud container clusters get-credentials $GKE_CLUSTER_NAME --zone $ZONE\n",
    "HOST_TEMP=!(kubectl describe configmap inverse-proxy-config -n $NAMESPACE | grep \"googleusercontent.com\")\n",
    "INVERSE_PROXY_HOSTNAME=HOST_TEMP[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data set to BQ\n",
    "Import the data set from cloud storage to BigQuery. A dataset is created and the table is imported under `covertype_data.covertype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Dataset 'marina-test-proj:covertype_dataset'\n",
      "already exists.\n",
      "Waiting on bqjob_r1b007fafba86bce9_000001788184eeaa_1 ... (5s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "DATASET_LOCATION='US'\n",
    "DATASET_ID='covertype_dataset'\n",
    "TABLE_ID='covertype'\n",
    "DATA_SOURCE='gs://workshop-datasets/covertype/full/dataset.csv'\n",
    "SCHEMA='Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER'\n",
    "\n",
    "!bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "!bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training application.\n",
    "Now that the data is hosted in BQ and we have created the KFP CLI Builder image, the next step is to create the training application. Start by creating the master pipeline folder and nested folders to host the model script, trainer image docker and the base image docker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mlops-demo/02_demo_CICD\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "#os.chdir('../02_demo_CICD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_APP_FOLDER ='pipeline'\n",
    "TRAINING_APP_FOLDER = 'pipeline/trainer_image'\n",
    "BASE_IMAGE_FOLDER='pipeline/base_image'\n",
    "os.makedirs(PIPELINE_APP_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)\n",
    "os.makedirs(BASE_IMAGE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the training script. \n",
    "\n",
    "The script written in the Experimentation Notebook, which process the data and trains the classification model, is written as a training script `train.py` in the training image folder. In addition to the model written during experimentation, an additional `hypertune` function is created which allows for a training job to be run with multiple parameters. This will  run multiple models with a range of parameters on CAIP training platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline/trainer_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\"\"\"Covertype Classifier trainer script.\"\"\"\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "  df_train = pd.read_csv(training_dataset_path)\n",
    "  df_validation = pd.read_csv(validation_dataset_path)\n",
    "    \n",
    "  if not hptune:\n",
    "    df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "  numeric_feature_indexes = slice(0, 10)\n",
    "  categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "  preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "  pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "    \n",
    "  num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "  df_train = df_train.astype(num_features_type_map)\n",
    "  df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "  print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "  X_train = df_train.drop('Cover_Type', axis=1)\n",
    "  y_train = df_train['Cover_Type']\n",
    "  \n",
    "  pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  \n",
    "  if hptune:\n",
    "    X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "    y_validation = df_validation['Cover_Type']\n",
    "    accuracy = pipeline.score(X_validation, y_validation)\n",
    "    print('Model accuracy: {}'.format(accuracy))\n",
    "    # Log it with hypertune\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='accuracy',\n",
    "      metric_value=accuracy\n",
    "    )\n",
    "\n",
    "  # Save the model\n",
    "  if not hptune:\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "    print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write pipeline script and accompanying helper components \n",
    "Now that we have images with the base dependancies and the training script built and saved into the container registry, the next step is to write the pipeline script `covertype_training_pipeline.py` which defines the pipeline which will be deployed to KFP. This file sets hypertuning settings and uses both pre-built components and custome built components defined in a seperate `helper_components.py`.  \n",
    "\n",
    "\n",
    "- Pre-build components. The pipeline uses the following pre-build components that are included with KFP distribution:\n",
    "    - BigQuery query component\n",
    "    - AI Platform Training component\n",
    "    - AI Platform Deploy component\n",
    "- Custom components. The pipeline uses two custom helper components that encapsulate functionality not available in any of the pre-build components. The components are implemented using the KFP SDK's Lightweight Python Components mechanism. The code for the components is in the helper_components.py file:\n",
    "    - Retrieve Best Run- This component retrieves the tuning metric and hyperparameter values for the best run of the AI Platform Training hyperparameter tuning job.\n",
    "    - Evaluate Model - This component evaluates the sklearn trained model using a provided metric and a testing dataset.\n",
    "\n",
    "The workflow implemented by the pipeline is defined using a Python based KFP Domain Specific Language (DSL). The pipeline's DSL is in the covertype_training_pipeline.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline/covertype_training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PIPELINE_APP_FOLDER}/covertype_training_pipeline.py\n",
    "\n",
    "\"\"\"KFP pipeline orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "\n",
    "\n",
    "TRAINING_FILE_PATH = 'datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 6,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "  \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "  sampling_query_template = \"\"\"\n",
    "       SELECT *\n",
    "       FROM \n",
    "           `{{ source_table }}` AS cover\n",
    "       WHERE \n",
    "       MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "       \"\"\"\n",
    "  query = Template(sampling_query_template).render(\n",
    "      source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "  return query\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_train(project_id: GCPProjectID,\n",
    "                    region: GCPRegion,\n",
    "                    source_table_name: String,\n",
    "                    gcs_root: GCSPath,\n",
    "                    dataset_id: str,\n",
    "                    evaluation_metric_name: str,\n",
    "                    evaluation_metric_threshold: float,\n",
    "                    model_id: str,\n",
    "                    version_id: str,\n",
    "                    replace_existing_version: bool,\n",
    "                    hypertune_settings: Dict = HYPERTUNE_SETTINGS,\n",
    "                    dataset_location: str = 'US'):\n",
    "  \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "  # Create the training split\n",
    "  query = generate_sampling_query(\n",
    "      source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "  training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "  create_training_split = bigquery_query_op(\n",
    "      query=query,\n",
    "      project_id=project_id,\n",
    "      dataset_id=dataset_id,\n",
    "      table_id='',\n",
    "      output_gcs_path=training_file_path,\n",
    "      dataset_location=dataset_location)\n",
    "\n",
    "  # Create the validation split\n",
    "  query = generate_sampling_query(\n",
    "      source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "  validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "  create_validation_split = bigquery_query_op(\n",
    "      query=query,\n",
    "      project_id=project_id,\n",
    "      dataset_id=dataset_id,\n",
    "      table_id='',\n",
    "      output_gcs_path=validation_file_path,\n",
    "      dataset_location=dataset_location)\n",
    "\n",
    "  # Create the testing split\n",
    "  query = generate_sampling_query(\n",
    "      source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "  testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "  create_testing_split = bigquery_query_op(\n",
    "      query=query,\n",
    "      project_id=project_id,\n",
    "      dataset_id=dataset_id,\n",
    "      table_id='',\n",
    "      output_gcs_path=testing_file_path,\n",
    "      dataset_location=dataset_location)\n",
    "\n",
    "  # Tune hyperparameters\n",
    "  tune_args = [\n",
    "      '--training_dataset_path',\n",
    "      create_training_split.outputs['output_gcs_path'],\n",
    "      '--validation_dataset_path',\n",
    "      create_validation_split.outputs['output_gcs_path'], '--hptune', 'True'\n",
    "  ]\n",
    "\n",
    "  job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                              kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "  hypertune = mlengine_train_op(\n",
    "      project_id=project_id,\n",
    "      region=region,\n",
    "      master_image_uri=TRAINER_IMAGE,\n",
    "      job_dir=job_dir,\n",
    "      args=tune_args,\n",
    "      training_input=hypertune_settings)\n",
    "\n",
    "  # Retrieve the best trial\n",
    "  get_best_trial = retrieve_best_run_op(project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "  # Train the model on a combined training and validation datasets\n",
    "  job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "  train_args = [\n",
    "      '--training_dataset_path',\n",
    "      create_training_split.outputs['output_gcs_path'],\n",
    "      '--validation_dataset_path',\n",
    "      create_validation_split.outputs['output_gcs_path'], '--alpha',\n",
    "      get_best_trial.outputs['alpha'], '--max_iter',\n",
    "      get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "  ]\n",
    "\n",
    "  train_model = mlengine_train_op(\n",
    "      project_id=project_id,\n",
    "      region=region,\n",
    "      master_image_uri=TRAINER_IMAGE,\n",
    "      job_dir=job_dir,\n",
    "      args=train_args)\n",
    "\n",
    "  # Evaluate the model on the testing split\n",
    "  eval_model = evaluate_model_op(\n",
    "      dataset_path=str(create_testing_split.outputs['output_gcs_path']),\n",
    "      model_path=str(train_model.outputs['job_dir']),\n",
    "      metric_name=evaluation_metric_name)\n",
    "\n",
    "  # Deploy the model if the primary metric is better than threshold\n",
    "  with kfp.dsl.Condition(\n",
    "      eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "    deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "  kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the custom 'helper' components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline/helper_components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PIPELINE_APP_FOLDER}/helper_components.py\n",
    "\n",
    "\"\"\"Helper components.\"\"\"\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(\n",
    "    project_id: str, job_id: str\n",
    ") -> NamedTuple('Outputs', [('metric_value', float), ('alpha', float),\n",
    "                            ('max_iter', int)]):\n",
    "  \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "  from googleapiclient import discovery\n",
    "  from googleapiclient import errors\n",
    "\n",
    "  ml = discovery.build('ml', 'v1')\n",
    "\n",
    "  job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "  request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "  try:\n",
    "    response = request.execute()\n",
    "  except errors.HttpError as err:\n",
    "    print(err)\n",
    "  except:\n",
    "    print('Unexpected error')\n",
    "\n",
    "  print(response)\n",
    "\n",
    "  best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "  metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "  alpha = float(best_trial['hyperparameters']['alpha'])\n",
    "  max_iter = int(best_trial['hyperparameters']['max_iter'])\n",
    "\n",
    "  return (metric_value, alpha, max_iter)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    dataset_path: str, model_path: str, metric_name: str\n",
    ") -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "  \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "  #import joblib\n",
    "  import pickle\n",
    "  import json\n",
    "  import pandas as pd\n",
    "  import subprocess\n",
    "  import sys\n",
    "\n",
    "  from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "  df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "  X_test = df_test.drop('Cover_Type', axis=1)\n",
    "  y_test = df_test['Cover_Type']\n",
    "\n",
    "  # Copy the model from GCS\n",
    "  model_filename = 'model.pkl'\n",
    "  gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "  print(gcs_model_filepath)\n",
    "  subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n",
    "                        stderr=sys.stdout)\n",
    "\n",
    "  with open(model_filename, 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "  y_hat = model.predict(X_test)\n",
    "\n",
    "  if metric_name == 'accuracy':\n",
    "    metric_value = accuracy_score(y_test, y_hat)\n",
    "  elif metric_name == 'recall':\n",
    "    metric_value = recall_score(y_test, y_hat)\n",
    "  else:\n",
    "    metric_name = 'N/A'\n",
    "    metric_value = 0\n",
    "\n",
    "  # Export the metric\n",
    "  metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "  }\n",
    "\n",
    "  return (metric_name, metric_value, json.dumps(metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating KFP CLI builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile \n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install https://storage.googleapis.com/ml-pipeline/release/0.1.36/kfp.tar.gz \n",
    "\n",
    "ENTRYPOINT [\"/bin/bash\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 6 file(s) totalling 87.3 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://marina-test-proj_cloudbuild/source/1617080849.6-e4815079450742d781e0fbe7960a32a6.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/marina-test-proj/builds/bfba6d74-7ce6-47fc-b987-c486b91a0b17].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/bfba6d74-7ce6-47fc-b987-c486b91a0b17?project=989115307566].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"bfba6d74-7ce6-47fc-b987-c486b91a0b17\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://marina-test-proj_cloudbuild/source/1617080849.6-e4815079450742d781e0fbe7960a32a6.tgz#1617080849827269\n",
      "Copying gs://marina-test-proj_cloudbuild/source/1617080849.6-e4815079450742d781e0fbe7960a32a6.tgz#1617080849827269...\n",
      "/ [1 files][ 19.4 KiB/ 19.4 KiB]                                                \n",
      "Operation completed over 1 objects/19.4 KiB.                                     \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  96.77kB\n",
      "Step 1/3 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d519e2592276: Pulling fs layer\n",
      "d22d2dfcfa9c: Pulling fs layer\n",
      "b3afe92c540b: Pulling fs layer\n",
      "33db9b363c09: Pulling fs layer\n",
      "0a0885dd1f38: Pulling fs layer\n",
      "19b22bb5c9e5: Pulling fs layer\n",
      "6baf8923035d: Pulling fs layer\n",
      "7d5e0de771e2: Pulling fs layer\n",
      "15a0038d8b0b: Pulling fs layer\n",
      "ac486096998c: Pulling fs layer\n",
      "8d9eec950e64: Pulling fs layer\n",
      "d850ab836f6c: Pulling fs layer\n",
      "f03f403bedb2: Pulling fs layer\n",
      "2708198982c3: Pulling fs layer\n",
      "c26d0bc41a70: Pulling fs layer\n",
      "1142fe36afbd: Pulling fs layer\n",
      "963bd36f7e26: Pulling fs layer\n",
      "3aeb5968a821: Pulling fs layer\n",
      "33db9b363c09: Waiting\n",
      "0a0885dd1f38: Waiting\n",
      "19b22bb5c9e5: Waiting\n",
      "6baf8923035d: Waiting\n",
      "7d5e0de771e2: Waiting\n",
      "15a0038d8b0b: Waiting\n",
      "ac486096998c: Waiting\n",
      "8d9eec950e64: Waiting\n",
      "d850ab836f6c: Waiting\n",
      "f03f403bedb2: Waiting\n",
      "2708198982c3: Waiting\n",
      "c26d0bc41a70: Waiting\n",
      "3aeb5968a821: Waiting\n",
      "1142fe36afbd: Waiting\n",
      "963bd36f7e26: Waiting\n",
      "b3afe92c540b: Verifying Checksum\n",
      "b3afe92c540b: Download complete\n",
      "d22d2dfcfa9c: Download complete\n",
      "33db9b363c09: Verifying Checksum\n",
      "33db9b363c09: Download complete\n",
      "d519e2592276: Verifying Checksum\n",
      "d519e2592276: Download complete\n",
      "6baf8923035d: Verifying Checksum\n",
      "6baf8923035d: Download complete\n",
      "19b22bb5c9e5: Verifying Checksum\n",
      "19b22bb5c9e5: Download complete\n",
      "15a0038d8b0b: Verifying Checksum\n",
      "15a0038d8b0b: Download complete\n",
      "ac486096998c: Verifying Checksum\n",
      "ac486096998c: Download complete\n",
      "8d9eec950e64: Verifying Checksum\n",
      "8d9eec950e64: Download complete\n",
      "d850ab836f6c: Verifying Checksum\n",
      "d850ab836f6c: Download complete\n",
      "f03f403bedb2: Verifying Checksum\n",
      "f03f403bedb2: Download complete\n",
      "2708198982c3: Verifying Checksum\n",
      "2708198982c3: Download complete\n",
      "7d5e0de771e2: Verifying Checksum\n",
      "7d5e0de771e2: Download complete\n",
      "c26d0bc41a70: Verifying Checksum\n",
      "c26d0bc41a70: Download complete\n",
      "1142fe36afbd: Verifying Checksum\n",
      "1142fe36afbd: Download complete\n",
      "3aeb5968a821: Verifying Checksum\n",
      "3aeb5968a821: Download complete\n",
      "0a0885dd1f38: Verifying Checksum\n",
      "0a0885dd1f38: Download complete\n",
      "d519e2592276: Pull complete\n",
      "d22d2dfcfa9c: Pull complete\n",
      "b3afe92c540b: Pull complete\n",
      "33db9b363c09: Pull complete\n",
      "963bd36f7e26: Verifying Checksum\n",
      "963bd36f7e26: Download complete\n",
      "0a0885dd1f38: Pull complete\n",
      "19b22bb5c9e5: Pull complete\n",
      "6baf8923035d: Pull complete\n",
      "7d5e0de771e2: Pull complete\n",
      "15a0038d8b0b: Pull complete\n",
      "ac486096998c: Pull complete\n",
      "8d9eec950e64: Pull complete\n",
      "d850ab836f6c: Pull complete\n",
      "f03f403bedb2: Pull complete\n",
      "2708198982c3: Pull complete\n",
      "c26d0bc41a70: Pull complete\n",
      "1142fe36afbd: Pull complete\n",
      "963bd36f7e26: Pull complete\n",
      "3aeb5968a821: Pull complete\n",
      "Digest: sha256:643f9241f318b695201de1c01b463590c940a4cff15e9505c3d0481e9aae5b8d\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> a583bf9d1534\n",
      "Step 2/3 : RUN pip install https://storage.googleapis.com/ml-pipeline/release/0.1.36/kfp.tar.gz\n",
      " ---> Running in d2e8949ad213\n",
      "Collecting https://storage.googleapis.com/ml-pipeline/release/0.1.36/kfp.tar.gz\n",
      "  Downloading https://storage.googleapis.com/ml-pipeline/release/0.1.36/kfp.tar.gz (111 kB)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (1.15.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (2.8.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (5.4.1)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (1.30.0)\n",
      "Collecting kubernetes<=9.0.0,>=8.0.0\n",
      "  Downloading kubernetes-9.0.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (2.0.1)\n",
      "Requirement already satisfied: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (3.4.4)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (1.24.0)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.25,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.18.3.tar.gz (33 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.1.36) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=2.4.2->kfp==0.1.36) (2.20)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.36) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.36) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.36) (4.2.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.36) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.36) (1.3.0)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.36) (1.2.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (1.22.4)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (3.15.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (1.52.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (2.25.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (2021.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (1.1.2)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.1.36) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.1.36) (3.7.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.1.36) (20.3.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.36) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.36) (0.57.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.1.36) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (2.10)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.1.36) (1.12.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp==0.1.36) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp==0.1.36) (3.7.4.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.36) (3.0.1)\n",
      "Building wheels for collected packages: kfp, argo-models, tabulate, kfp-server-api\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.1.36-py3-none-any.whl size=153785 sha256=7a3e5e9329125d7507da14fe48178bffdcf98f7e584ccab0bb62495d5f7790dc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ge5pnl1q/wheels/24/25/4a/33ff6c5e30b05c4aa30ede043b7723c995d59bd1b86260d5b7\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57308 sha256=52c2c996971da035b0699a23872aafcaa8b5226e21de850665f6e8bf3d836a76\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23379 sha256=99410ecb99493c9aa942bd04a921f75c73f90e0d7e3b1d5254555cde3fb173df\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.18.3-py3-none-any.whl size=95462 sha256=3399c212c6b27fec3682277722601bd62f02a470f5e960a3fbd55101196fa1b4\n",
      "  Stored in directory: /root/.cache/pip/wheels/39/03/4b/de0ea7cfcc4c85efef18cad6307caddb511333419588996c8c\n",
      "Successfully built kfp argo-models tabulate kfp-server-api\n",
      "Installing collected packages: urllib3, kubernetes, tabulate, requests-toolbelt, kfp-server-api, Deprecated, cloudpickle, click, argo-models, kfp\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.3\n",
      "    Uninstalling urllib3-1.26.3:\n",
      "      Successfully uninstalled urllib3-1.26.3\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 12.0.1\n",
      "    Uninstalling kubernetes-12.0.1:\n",
      "      Successfully uninstalled kubernetes-12.0.1\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.6.0\n",
      "    Uninstalling cloudpickle-1.6.0:\n",
      "      Successfully uninstalled cloudpickle-1.6.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "Successfully installed Deprecated-1.2.12 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 kfp-0.1.36 kfp-server-api-0.1.18.3 kubernetes-9.0.0 requests-toolbelt-0.9.1 tabulate-0.8.3 urllib3-1.24.3\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda 4.9.2 requires ruamel_yaml>=0.11.14, which is not installed.\n",
      "jupyterlab-git 0.11.0 requires nbdime<2.0.0,>=1.1.0, but you have nbdime 2.1.0 which is incompatible.\n",
      "black 20.8b1 requires click>=7.1.2, but you have click 7.0 which is incompatible.\n",
      "\u001b[0mRemoving intermediate container d2e8949ad213\n",
      " ---> 4171ceb44704\n",
      "Step 3/3 : ENTRYPOINT [\"/bin/bash\"]\n",
      " ---> Running in 317f1b4c3cf5\n",
      "Removing intermediate container 317f1b4c3cf5\n",
      " ---> b5a7414d39d3\n",
      "Successfully built b5a7414d39d3\n",
      "Successfully tagged gcr.io/marina-test-proj/kfp-cli:latest\n",
      "PUSH\n",
      "Pushing gcr.io/marina-test-proj/kfp-cli:latest\n",
      "The push refers to repository [gcr.io/marina-test-proj/kfp-cli]\n",
      "e86567316c37: Preparing\n",
      "ffcee209896b: Preparing\n",
      "e1f95cdc5308: Preparing\n",
      "1bb081ba2f36: Preparing\n",
      "a93b3053322a: Preparing\n",
      "12c5989adbbb: Preparing\n",
      "85cedf4cdbb0: Preparing\n",
      "14b51b86b48b: Preparing\n",
      "027c310aec4d: Preparing\n",
      "9067df2cd2c3: Preparing\n",
      "77daaf4d7fed: Preparing\n",
      "c8094967f023: Preparing\n",
      "606aacd1e9f0: Preparing\n",
      "7fae9d57911c: Preparing\n",
      "2002bd7cf301: Preparing\n",
      "92b46988c43f: Preparing\n",
      "9f10818f1f96: Preparing\n",
      "27502392e386: Preparing\n",
      "c95d2191d777: Preparing\n",
      "12c5989adbbb: Waiting\n",
      "85cedf4cdbb0: Waiting\n",
      "14b51b86b48b: Waiting\n",
      "027c310aec4d: Waiting\n",
      "9067df2cd2c3: Waiting\n",
      "77daaf4d7fed: Waiting\n",
      "c8094967f023: Waiting\n",
      "606aacd1e9f0: Waiting\n",
      "7fae9d57911c: Waiting\n",
      "2002bd7cf301: Waiting\n",
      "92b46988c43f: Waiting\n",
      "9f10818f1f96: Waiting\n",
      "27502392e386: Waiting\n",
      "c95d2191d777: Waiting\n",
      "a93b3053322a: Layer already exists\n",
      "e1f95cdc5308: Layer already exists\n",
      "1bb081ba2f36: Layer already exists\n",
      "ffcee209896b: Layer already exists\n",
      "027c310aec4d: Layer already exists\n",
      "85cedf4cdbb0: Layer already exists\n",
      "12c5989adbbb: Layer already exists\n",
      "14b51b86b48b: Layer already exists\n",
      "9067df2cd2c3: Layer already exists\n",
      "606aacd1e9f0: Layer already exists\n",
      "77daaf4d7fed: Layer already exists\n",
      "c8094967f023: Layer already exists\n",
      "7fae9d57911c: Layer already exists\n",
      "92b46988c43f: Layer already exists\n",
      "2002bd7cf301: Layer already exists\n",
      "9f10818f1f96: Layer already exists\n",
      "c95d2191d777: Layer already exists\n",
      "27502392e386: Layer already exists\n",
      "e86567316c37: Pushed\n",
      "latest: digest: sha256:15f732bf1d73074435766396ecf81e97cd30c67e848dd88870df20054b5da984 size: 4292\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                     IMAGES                                     STATUS\n",
      "bfba6d74-7ce6-47fc-b987-c486b91a0b17  2021-03-30T05:07:29+00:00  2M14S     gs://marina-test-proj_cloudbuild/source/1617080849.6-e4815079450742d781e0fbe7960a32a6.tgz  gcr.io/marina-test-proj/kfp-cli (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME='kfp-cli'\n",
    "IMAGE_URI=\"gcr.io/{}/{}:latest\".format(PROJECT_ID,IMAGE_NAME)\n",
    "!gcloud builds submit --timeout 15m --tag {IMAGE_URI} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "The docker images used for the training are based off the image `mlops-dev:latest` created during the inital set up of the environment. Since the AI Platform Notebook instance is based on the `mlops-dev:latest` image we use the same image as a base for the training image. \n",
    "\n",
    "\n",
    "We first write a base image dockerfile which replicates the image used for the Notebook. Then we write a training dockerfile which uses the same base image and add the `train.py` to the image. \n",
    "\n",
    "\n",
    "**NOTE:** Make sure to update the URI for the image so that it points to your project's **Container Registry**. i.e. `FROM gcr.io/PROJECT_ID/kfp-cli:latest` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline/trainer_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/marinas-demo/mlops-dev:latest\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline/base_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {BASE_IMAGE_FOLDER}/Dockerfile\n",
    "FROM gcr.io/marinas-demo/mlops-dev:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cloud build config file\n",
    "This file is used by **Cloud Build** to create all container images required and deploys the pipeline to kubeflow pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "# Build the trainer image\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/trainer_image\n",
    "  \n",
    "# Build the base image for lightweight components\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/base_image\n",
    "\n",
    "# Compile the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    dsl-compile --py $_PIPELINE_DSL --output $_PIPELINE_PACKAGE\n",
    "  env:\n",
    "  - 'BASE_IMAGE=gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'TRAINER_IMAGE=gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'RUNTIME_VERSION=$_RUNTIME_VERSION'\n",
    "  - 'PYTHON_VERSION=$_PYTHON_VERSION'\n",
    "  - 'COMPONENT_URL_SEARCH_PREFIX=$_COMPONENT_URL_SEARCH_PREFIX'\n",
    "  dir: $_PIPELINE_FOLDER\n",
    "  \n",
    " # Upload the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    kfp --endpoint $_INVERTING_PROXY_HOST pipeline upload -p ${_PIPELINE_NAME}_$TAG_NAME $_PIPELINE_PACKAGE\n",
    "  dir: $_PIPELINE_FOLDER\n",
    "\n",
    "\n",
    "# Push the images to Container Registry \n",
    "images: ['gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', 'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit to Cloud Source Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Source Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME='{}-test'.format(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open terminal in Jupyter notebook and set up an authentication channel to cloud source repo. Run the gcloud init command and follow prompts, create a new source repo with the name of your project ID  \n",
    "`gcloud init && git config credential.helper gcloud.sh` - select option 2, then yes, follow the link, copy the verification code back in the terminal command. Once verified, select [1] Enter project ID and type your project ID, then select no for configure region.\n",
    "\n",
    "`gcloud source repos create REPO_NAME` - you will need to specify your REPO_NAME \n",
    "\n",
    "*This step is simpler if done in terminal*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: remote marina-test-proj-test already exists.\n"
     ]
    }
   ],
   "source": [
    "!cd /home/mlops-demo\n",
    "!git remote add {REPO_NAME} https://source.developers.google.com/p/{PROJECT_ID}/r/{REPO_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is ahead of 'origin/master' by 2 commits.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "Changes not staged for commit:\n",
      "\t\u001b[31mmodified:   ../01_Experimentation.ipynb\u001b[m\n",
      "\n",
      "no changes added to commit\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Upload ML\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: remote error: Git repository not found\n"
     ]
    }
   ],
   "source": [
    "!git push --all {REPO_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cloud Build trigger\n",
    "This will cause the images to build and will deploy the pipeline apon a push to the source repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a trigger build configuration file\n",
    "This file is used by cloud trigger, to pass the variables required by cloud build. \n",
    "\n",
    "**NOTE** Ensure to change the `Project_ID`, `repoName` and `_INVERTING_PROXY_HOST` variables to reflect your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(INVERSE_PROXY_HOSTNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trigger_build.yaml\n",
    "\n",
    "description: triggers cloud build upon push\n",
    "name: trigger-build\n",
    "filename: 02_demo_CICD/cloudbuild.yaml\n",
    "triggerTemplate:\n",
    "    projectId: marinas-demo\n",
    "    repoName: marinas-demo-test\n",
    "    branchName: master\n",
    "substitutions:\n",
    "    _INVERTING_PROXY_HOST: 4bcc62bc46b37077-dot-us-central1.notebooks.googleusercontent.com\n",
    "    _TRAINER_IMAGE_NAME: trainer_image\n",
    "    _BASE_IMAGE_NAME: base_image\n",
    "    TAG_NAME: test\n",
    "    _PIPELINE_FOLDER: 02_demo_CICD/pipeline\n",
    "    _PIPELINE_DSL: covertype_training_pipeline.py\n",
    "    _PIPELINE_PACKAGE: covertype_training_pipeline.yaml\n",
    "    _PIPELINE_NAME: covertype_training_deployment\n",
    "    _RUNTIME_VERSION: \"1.14\"\n",
    "    _PYTHON_VERSION: \"3.5\"\n",
    "    _COMPONENT_URL_SEARCH_PREFIX: https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command creates a **Cloud Build** trigger upon any new code committed to the master of our repository. \n",
    "\n",
    "**NOTE**: Only run this command once or it will create multiple triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta builds triggers create cloud-source-repositories --trigger-config=trigger_build.yaml --project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run trigger on push\n",
    "We will now push code to our repository, which will automatically trigger a cloud build of the new container images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"Upload ML\"\n",
    "!git push --all {REPO_NAME}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds list --ongoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} pipeline list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the pipeline\n",
    "The deployed pipeline can be viewed through the Kubeflow Pipeline UI given at the URL below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('https://{}'.format(INVERSE_PROXY_HOSTNAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment \n",
    "Now that the pipeline is deployed we want to run an experiment, this will cause the pipeline to run, pulling the data from bigquery and splitting it, training the models, evaluating them and deploy the best performing model. This experiment takes approximately an hour to execute and will result in a deployed model which can be interacted with through GCP's AI platform predicting service. \n",
    "\n",
    "**NOTE:** Change the PIPELINE_ID to reflect the ID copied from above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='ab90cc6c-3f00-480c-bea3-bd1959e7394b'\n",
    "\n",
    "EXPERIMENT_NAME='Covertype_Classifier_Training'\n",
    "RUN_ID='Run_001'\n",
    "SOURCE_TABLE='covertype_dataset.covertype'\n",
    "DATASET_ID='splits'\n",
    "EVALUATION_METRIC='accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD='0.69'\n",
    "MODEL_ID='covertype_classifier'\n",
    "VERSION_ID='v01'\n",
    "REPLACE_EXISTING_VERSION=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} run submit \\\n",
    "-e Covertype_Classifier_Training \\\n",
    "-r {RUN_ID} \\\n",
    "-p {PIPELINE_ID} \\\n",
    "project_id={PROJECT_ID} \\\n",
    "gcs_root={GCS_STAGING_PATH} \\\n",
    "region={REGION} \\\n",
    "source_table_name={SOURCE_TABLE} \\\n",
    "dataset_id={DATASET_ID} \\\n",
    "evaluation_metric_name={EVALUATION_METRIC} \\\n",
    "evaluation_metric_threshold={EVALUATION_METRIC_THRESHOLD} \\\n",
    "model_id={MODEL_ID} \\\n",
    "version_id={VERSION_ID} \\\n",
    "replace_existing_version={REPLACE_EXISTING_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model\n",
    "To test the model we can use the AI platforms prediction API to ask for a prediction based on a JSON input aternatively we can use the prediction UI and input: *{\"instances\":[[2395,0,0,60,6,1170,218,238,156,1054,\"Cache\",\"C2717\"]]}* in the test case window.\n",
    "\n",
    "We write a prediction JSON file with a set of data points, the correct cover types are 3 and 2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predict.json\n",
    "[2395,0,0,60,6,1170,218,238,156,1054,\"Cache\",\"C2717\"]\n",
    "[2756,135,0,85,14,1608,219,238,156,2451,\"Rawah\",\"C4744\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE=\"./predict.json\"\n",
    "\n",
    "!gcloud ai-platform predict --model {MODEL_ID} \\\n",
    "  --version {VERSION_ID} \\\n",
    "  --json-instances {INPUT_DATA_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r pipeline/ cloudbuild.yaml build_pipeline.sh Dockerfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
