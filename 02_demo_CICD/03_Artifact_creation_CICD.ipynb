{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Productionising the ML model\n",
    "This notebook will walk through the process of creating, building and commiting the artifacts required to run the model developed in the Experimentation Notebook in production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "**NOTE:** Set Project ID to your project  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'marinas-demo'\n",
    "PREFIX = PROJECT_ID\n",
    "REGION = 'us-central1'\n",
    "\n",
    "DATA_ROOT = 'gs://workshop-datasets/covertype'\n",
    "TRAINING_FILE_PATH = DATA_ROOT + '/training/dataset.csv'\n",
    "VALIDATION_FILE_PATH = DATA_ROOT + '/evaluation/dataset.csv'\n",
    "\n",
    "# Job dir for AI Platform Training\n",
    "JOB_DIR_ROOT='gs://{}-artifact-store/jobs'.format(PREFIX)\n",
    "\n",
    "\n",
    "NAMESPACE='kubeflow'\n",
    "ZONE='us-central1-a'\n",
    "ARTIFACT_STORE_URI='gs://{}-artifact-store'.format(PREFIX)\n",
    "GCS_STAGING_PATH='{}/staging'.format(ARTIFACT_STORE_URI)\n",
    "GKE_CLUSTER_NAME='{}-cluster'.format(PREFIX)\n",
    "\n",
    "!gcloud container clusters get-credentials $GKE_CLUSTER_NAME --zone $ZONE\n",
    "HOST_TEMP=!(kubectl describe configmap inverse-proxy-config -n $NAMESPACE | grep \"googleusercontent.com\")\n",
    "INVERSE_PROXY_HOSTNAME=HOST_TEMP[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data set to BQ\n",
    "Import the data set from cloud storage to BigQuery. A dataset is created and the table is imported under `covertype_data.covertype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LOCATION='US'\n",
    "DATASET_ID='covertype_dataset'\n",
    "TABLE_ID='covertype'\n",
    "DATA_SOURCE='gs://workshop-datasets/covertype/full/dataset.csv'\n",
    "SCHEMA='Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER'\n",
    "\n",
    "!bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "!bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training application.\n",
    "Now that the data is hosted in BQ and we have created the KFP CLI Builder image, the next step is to create the training application. Start by creating the master pipeline folder and nested folders to host the model script, trainer image docker and the base image docker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "#os.chdir('../02_demo_CICD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_APP_FOLDER ='pipeline'\n",
    "TRAINING_APP_FOLDER = 'pipeline/trainer_image'\n",
    "BASE_IMAGE_FOLDER='pipeline/base_image'\n",
    "os.makedirs(PIPELINE_APP_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)\n",
    "os.makedirs(BASE_IMAGE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the training script. \n",
    "\n",
    "The script written in the Experimentation Notebook, which process the data and trains the classification model, is written as a training script `train.py` in the training image folder. In addition to the model written during experimentation, an additional `hypertune` function is created which allows for a training job to be run with multiple parameters. This will  run multiple models with a range of parameters on CAIP training platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\"\"\"Covertype Classifier trainer script.\"\"\"\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "  df_train = pd.read_csv(training_dataset_path)\n",
    "  df_validation = pd.read_csv(validation_dataset_path)\n",
    "    \n",
    "  if not hptune:\n",
    "    df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "  numeric_feature_indexes = slice(0, 10)\n",
    "  categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "  preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "  pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "    \n",
    "  num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "  df_train = df_train.astype(num_features_type_map)\n",
    "  df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "  print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "  X_train = df_train.drop('Cover_Type', axis=1)\n",
    "  y_train = df_train['Cover_Type']\n",
    "  \n",
    "  pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  \n",
    "  if hptune:\n",
    "    X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "    y_validation = df_validation['Cover_Type']\n",
    "    accuracy = pipeline.score(X_validation, y_validation)\n",
    "    print('Model accuracy: {}'.format(accuracy))\n",
    "    # Log it with hypertune\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='accuracy',\n",
    "      metric_value=accuracy\n",
    "    )\n",
    "\n",
    "  # Save the model\n",
    "  if not hptune:\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "    print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write pipeline script and accompanying helper components \n",
    "Now that we have images with the base dependancies and the training script built and saved into the container registry, the next step is to write the pipeline script `covertype_training_pipeline.py` which defines the pipeline which will be deployed to KFP. This file sets hypertuning settings and uses both pre-built components and custome built components defined in a seperate `helper_components.py`.  \n",
    "\n",
    "\n",
    "- Pre-build components. The pipeline uses the following pre-build components that are included with KFP distribution:\n",
    "    - BigQuery query component\n",
    "    - AI Platform Training component\n",
    "    - AI Platform Deploy component\n",
    "- Custom components. The pipeline uses two custom helper components that encapsulate functionality not available in any of the pre-build components. The components are implemented using the KFP SDK's Lightweight Python Components mechanism. The code for the components is in the helper_components.py file:\n",
    "    - Retrieve Best Run- This component retrieves the tuning metric and hyperparameter values for the best run of the AI Platform Training hyperparameter tuning job.\n",
    "    - Evaluate Model - This component evaluates the sklearn trained model using a provided metric and a testing dataset.\n",
    "\n",
    "The workflow implemented by the pipeline is defined using a Python based KFP Domain Specific Language (DSL). The pipeline's DSL is in the covertype_training_pipeline.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PIPELINE_APP_FOLDER}/covertype_training_pipeline.py\n",
    "\n",
    "\"\"\"KFP pipeline orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "\n",
    "\n",
    "TRAINING_FILE_PATH = 'datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 6,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "  \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "  sampling_query_template = \"\"\"\n",
    "       SELECT *\n",
    "       FROM \n",
    "           `{{ source_table }}` AS cover\n",
    "       WHERE \n",
    "       MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "       \"\"\"\n",
    "  query = Template(sampling_query_template).render(\n",
    "      source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "  return query\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_train(project_id: GCPProjectID,\n",
    "                    region: GCPRegion,\n",
    "                    source_table_name: String,\n",
    "                    gcs_root: GCSPath,\n",
    "                    dataset_id: str,\n",
    "                    evaluation_metric_name: str,\n",
    "                    evaluation_metric_threshold: float,\n",
    "                    model_id: str,\n",
    "                    version_id: str,\n",
    "                    replace_existing_version: bool,\n",
    "                    hypertune_settings: Dict = HYPERTUNE_SETTINGS,\n",
    "                    dataset_location: str = 'US'):\n",
    "  \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "  # Create the training split\n",
    "  query = generate_sampling_query(\n",
    "      source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "  training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "  create_training_split = bigquery_query_op(\n",
    "      query=query,\n",
    "      project_id=project_id,\n",
    "      dataset_id=dataset_id,\n",
    "      table_id='',\n",
    "      output_gcs_path=training_file_path,\n",
    "      dataset_location=dataset_location)\n",
    "\n",
    "  # Create the validation split\n",
    "  query = generate_sampling_query(\n",
    "      source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "  validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "  create_validation_split = bigquery_query_op(\n",
    "      query=query,\n",
    "      project_id=project_id,\n",
    "      dataset_id=dataset_id,\n",
    "      table_id='',\n",
    "      output_gcs_path=validation_file_path,\n",
    "      dataset_location=dataset_location)\n",
    "\n",
    "  # Create the testing split\n",
    "  query = generate_sampling_query(\n",
    "      source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "  testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "  create_testing_split = bigquery_query_op(\n",
    "      query=query,\n",
    "      project_id=project_id,\n",
    "      dataset_id=dataset_id,\n",
    "      table_id='',\n",
    "      output_gcs_path=testing_file_path,\n",
    "      dataset_location=dataset_location)\n",
    "\n",
    "  # Tune hyperparameters\n",
    "  tune_args = [\n",
    "      '--training_dataset_path',\n",
    "      create_training_split.outputs['output_gcs_path'],\n",
    "      '--validation_dataset_path',\n",
    "      create_validation_split.outputs['output_gcs_path'], '--hptune', 'True'\n",
    "  ]\n",
    "\n",
    "  job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                              kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "  hypertune = mlengine_train_op(\n",
    "      project_id=project_id,\n",
    "      region=region,\n",
    "      master_image_uri=TRAINER_IMAGE,\n",
    "      job_dir=job_dir,\n",
    "      args=tune_args,\n",
    "      training_input=hypertune_settings)\n",
    "\n",
    "  # Retrieve the best trial\n",
    "  get_best_trial = retrieve_best_run_op(project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "  # Train the model on a combined training and validation datasets\n",
    "  job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "  train_args = [\n",
    "      '--training_dataset_path',\n",
    "      create_training_split.outputs['output_gcs_path'],\n",
    "      '--validation_dataset_path',\n",
    "      create_validation_split.outputs['output_gcs_path'], '--alpha',\n",
    "      get_best_trial.outputs['alpha'], '--max_iter',\n",
    "      get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "  ]\n",
    "\n",
    "  train_model = mlengine_train_op(\n",
    "      project_id=project_id,\n",
    "      region=region,\n",
    "      master_image_uri=TRAINER_IMAGE,\n",
    "      job_dir=job_dir,\n",
    "      args=train_args)\n",
    "\n",
    "  # Evaluate the model on the testing split\n",
    "  eval_model = evaluate_model_op(\n",
    "      dataset_path=str(create_testing_split.outputs['output_gcs_path']),\n",
    "      model_path=str(train_model.outputs['job_dir']),\n",
    "      metric_name=evaluation_metric_name)\n",
    "\n",
    "  # Deploy the model if the primary metric is better than threshold\n",
    "  with kfp.dsl.Condition(\n",
    "      eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "    deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "  kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the custom 'helper' components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PIPELINE_APP_FOLDER}/helper_components.py\n",
    "\n",
    "\"\"\"Helper components.\"\"\"\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(\n",
    "    project_id: str, job_id: str\n",
    ") -> NamedTuple('Outputs', [('metric_value', float), ('alpha', float),\n",
    "                            ('max_iter', int)]):\n",
    "  \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "  from googleapiclient import discovery\n",
    "  from googleapiclient import errors\n",
    "\n",
    "  ml = discovery.build('ml', 'v1')\n",
    "\n",
    "  job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "  request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "  try:\n",
    "    response = request.execute()\n",
    "  except errors.HttpError as err:\n",
    "    print(err)\n",
    "  except:\n",
    "    print('Unexpected error')\n",
    "\n",
    "  print(response)\n",
    "\n",
    "  best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "  metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "  alpha = float(best_trial['hyperparameters']['alpha'])\n",
    "  max_iter = int(best_trial['hyperparameters']['max_iter'])\n",
    "\n",
    "  return (metric_value, alpha, max_iter)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    dataset_path: str, model_path: str, metric_name: str\n",
    ") -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "  \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "  #import joblib\n",
    "  import pickle\n",
    "  import json\n",
    "  import pandas as pd\n",
    "  import subprocess\n",
    "  import sys\n",
    "\n",
    "  from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "  df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "  X_test = df_test.drop('Cover_Type', axis=1)\n",
    "  y_test = df_test['Cover_Type']\n",
    "\n",
    "  # Copy the model from GCS\n",
    "  model_filename = 'model.pkl'\n",
    "  gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "  print(gcs_model_filepath)\n",
    "  subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n",
    "                        stderr=sys.stdout)\n",
    "\n",
    "  with open(model_filename, 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "  y_hat = model.predict(X_test)\n",
    "\n",
    "  if metric_name == 'accuracy':\n",
    "    metric_value = accuracy_score(y_test, y_hat)\n",
    "  elif metric_name == 'recall':\n",
    "    metric_value = recall_score(y_test, y_hat)\n",
    "  else:\n",
    "    metric_name = 'N/A'\n",
    "    metric_value = 0\n",
    "\n",
    "  # Export the metric\n",
    "  metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "  }\n",
    "\n",
    "  return (metric_name, metric_value, json.dumps(metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating KFP CLI builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile \n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install https://storage.googleapis.com/ml-pipeline/release/0.1.36/kfp.tar.gz \n",
    "\n",
    "ENTRYPOINT [\"/bin/bash\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 7 file(s) totalling 85.6 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://marinas-demo_cloudbuild/source/1581993015.69-1da9d4434e664a739b7c3f4a4078683f.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/marinas-demo/builds/a822d5be-d9c6-4ab7-9a42-8392d7c4a0dc].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/a822d5be-d9c6-4ab7-9a42-8392d7c4a0dc?project=77376158912].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"a822d5be-d9c6-4ab7-9a42-8392d7c4a0dc\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://marinas-demo_cloudbuild/source/1581993015.69-1da9d4434e664a739b7c3f4a4078683f.tgz#1581993015967626\n",
      "Copying gs://marinas-demo_cloudbuild/source/1581993015.69-1da9d4434e664a739b7c3f4a4078683f.tgz#1581993015967626...\n",
      "/ [1 files][ 18.9 KiB/ 18.9 KiB]                                                \n",
      "Operation completed over 1 objects/18.9 KiB.                                     \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  96.26kB\n",
      "Step 1/3 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "5c939e3a4d10: Already exists\n",
      "c63719cdbe7a: Already exists\n",
      "19a861ea6baf: Already exists\n",
      "651c9d2d6c4f: Already exists\n",
      "82a164de4f2e: Pulling fs layer\n",
      "f03bf9c06e13: Pulling fs layer\n",
      "8509a446f7b8: Pulling fs layer\n",
      "81c250b3bc98: Pulling fs layer\n",
      "be5bf93118fa: Pulling fs layer\n",
      "2cfa78b49df3: Pulling fs layer\n",
      "25d82761132e: Pulling fs layer\n",
      "164864d0332d: Pulling fs layer\n",
      "e76f8a6ce9b4: Pulling fs layer\n",
      "25a7e9914e22: Pulling fs layer\n",
      "0c260d2de4b1: Pulling fs layer\n",
      "359e196fe5ef: Pulling fs layer\n",
      "81c250b3bc98: Waiting\n",
      "be5bf93118fa: Waiting\n",
      "2cfa78b49df3: Waiting\n",
      "25d82761132e: Waiting\n",
      "164864d0332d: Waiting\n",
      "e76f8a6ce9b4: Waiting\n",
      "25a7e9914e22: Waiting\n",
      "0c260d2de4b1: Waiting\n",
      "359e196fe5ef: Waiting\n",
      "8509a446f7b8: Verifying Checksum\n",
      "8509a446f7b8: Download complete\n",
      "f03bf9c06e13: Verifying Checksum\n",
      "f03bf9c06e13: Download complete\n",
      "81c250b3bc98: Verifying Checksum\n",
      "81c250b3bc98: Download complete\n",
      "2cfa78b49df3: Verifying Checksum\n",
      "2cfa78b49df3: Download complete\n",
      "25d82761132e: Verifying Checksum\n",
      "25d82761132e: Download complete\n",
      "164864d0332d: Verifying Checksum\n",
      "164864d0332d: Download complete\n",
      "e76f8a6ce9b4: Verifying Checksum\n",
      "e76f8a6ce9b4: Download complete\n",
      "25a7e9914e22: Verifying Checksum\n",
      "25a7e9914e22: Download complete\n",
      "82a164de4f2e: Verifying Checksum\n",
      "82a164de4f2e: Download complete\n",
      "0c260d2de4b1: Verifying Checksum\n",
      "0c260d2de4b1: Download complete\n",
      "359e196fe5ef: Verifying Checksum\n",
      "359e196fe5ef: Download complete\n",
      "be5bf93118fa: Verifying Checksum\n",
      "be5bf93118fa: Download complete\n",
      "82a164de4f2e: Pull complete\n",
      "f03bf9c06e13: Pull complete\n",
      "8509a446f7b8: Pull complete\n",
      "81c250b3bc98: Pull complete\n",
      "be5bf93118fa: Pull complete\n",
      "2cfa78b49df3: Pull complete\n",
      "25d82761132e: Pull complete\n",
      "164864d0332d: Pull complete\n",
      "e76f8a6ce9b4: Pull complete\n",
      "25a7e9914e22: Pull complete\n",
      "0c260d2de4b1: Pull complete\n",
      "359e196fe5ef: Pull complete\n",
      "Digest: sha256:6024b471264630a88f698248058ba192c4d301c4496dd1d83f750b9cca409d82\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d31a636f1f64\n",
      "Step 2/3 : RUN pip install https://storage.googleapis.com/ml-pipeline/release/0.1.36/kfp.tar.gz\n",
      " ---> Running in 8682e4c62fa6\n",
      "\u001b[91mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[0mCollecting https://storage.googleapis.com/ml-pipeline/release/0.1.36/kfp.tar.gz\n",
      "  Downloading https://storage.googleapis.com/ml-pipeline/release/0.1.36/kfp.tar.gz (111 kB)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (1.14.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (2.8.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (5.3)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (1.25.0)\n",
      "Collecting kubernetes<=9.0.0,>=8.0.0\n",
      "  Downloading kubernetes-9.0.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (1.7.1)\n",
      "Requirement already satisfied: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (2.8)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (1.10.2)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.25,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.18.3.tar.gz (33 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Requirement already satisfied: click==7.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.36) (7.0)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.7-py2.py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.36) (1.2.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.36) (0.5.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.36) (2.22.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.36) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.36) (1.2.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.36) (45.1.0.post20200119)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.1.36) (1.13.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.36) (0.2.7)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.36) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.36) (3.1.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.1.36) (0.15.7)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.1.36) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.1.36) (19.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.1.36) (1.11.2)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (1.16.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.36) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.36) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.36) (3.0.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.1.36) (2.19)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.1.36) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.1.36) (2.1.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (2019.3)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (3.11.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.36) (1.51.0)\n",
      "Building wheels for collected packages: kfp, kfp-server-api, argo-models, tabulate\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.1.36-py3-none-any.whl size=153785 sha256=ad56f50b98775bb8791b836ab9691e3e729e78f438a519217c0e2e26569fee9b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nf41ip_t/wheels/24/25/4a/33ff6c5e30b05c4aa30ede043b7723c995d59bd1b86260d5b7\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.18.3-py3-none-any.whl size=95462 sha256=c7dcb03fab15e851807354a26d48fe4a60b4c511e483608509cd59e7ff422ee9\n",
      "  Stored in directory: /root/.cache/pip/wheels/39/03/4b/de0ea7cfcc4c85efef18cad6307caddb511333419588996c8c\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57306 sha256=be9398f24435980f3f17a88b6b611119fb26e1cdb4fd4b44af7d0866287e0524\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23379 sha256=21dfb27dc18a313a34f1c5cf93b4dd6712069230924e461ef377fbc1996730df\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "Successfully built kfp kfp-server-api argo-models tabulate\n",
      "\u001b[91mERROR: fairing 0.5.3 has requirement tornado<6.0.0,>=5.1.1, but you'll have tornado 6.0.3 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: fairing 0.5.3 has requirement urllib3==1.24.2, but you'll have urllib3 1.24.3 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: datalab 1.1.5 has requirement pandas-profiling==1.4.0, but you'll have pandas-profiling 2.4.0 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, Deprecated, kfp\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.7\n",
      "    Uninstalling urllib3-1.25.7:\n",
      "      Successfully uninstalled urllib3-1.25.7\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 10.0.1\n",
      "    Uninstalling kubernetes-10.0.1:\n",
      "      Successfully uninstalled kubernetes-10.0.1\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.2.2\n",
      "    Uninstalling cloudpickle-1.2.2:\n",
      "      Successfully uninstalled cloudpickle-1.2.2\n",
      "Successfully installed Deprecated-1.2.7 argo-models-2.2.1a0 cloudpickle-1.1.1 kfp-0.1.36 kfp-server-api-0.1.18.3 kubernetes-9.0.0 requests-toolbelt-0.9.1 tabulate-0.8.3 urllib3-1.24.3\n",
      "Removing intermediate container 8682e4c62fa6\n",
      " ---> 725ba02da811\n",
      "Step 3/3 : ENTRYPOINT [\"/bin/bash\"]\n",
      " ---> Running in c220a9c083ea\n",
      "Removing intermediate container c220a9c083ea\n",
      " ---> 0ec7f935fdb5\n",
      "Successfully built 0ec7f935fdb5\n",
      "Successfully tagged gcr.io/marinas-demo/kfp-cli:latest\n",
      "PUSH\n",
      "Pushing gcr.io/marinas-demo/kfp-cli:latest\n",
      "The push refers to repository [gcr.io/marinas-demo/kfp-cli]\n",
      "34574cdf935e: Preparing\n",
      "c9ed5921085c: Preparing\n",
      "4c6f059d1398: Preparing\n",
      "f0faf772f9d3: Preparing\n",
      "f2aae1b4a02a: Preparing\n",
      "079a4b4488d4: Preparing\n",
      "801869434e6d: Preparing\n",
      "039262d27835: Preparing\n",
      "6f8ba6b2269e: Preparing\n",
      "67b2302cd884: Preparing\n",
      "448e01508468: Preparing\n",
      "bb7ac01267bd: Preparing\n",
      "430ca7c3fafc: Preparing\n",
      "f55aa0bd26b8: Preparing\n",
      "1d0dfb259f6a: Preparing\n",
      "21ec61b65b20: Preparing\n",
      "43c67172d1d1: Preparing\n",
      "079a4b4488d4: Waiting\n",
      "801869434e6d: Waiting\n",
      "039262d27835: Waiting\n",
      "6f8ba6b2269e: Waiting\n",
      "67b2302cd884: Waiting\n",
      "448e01508468: Waiting\n",
      "bb7ac01267bd: Waiting\n",
      "430ca7c3fafc: Waiting\n",
      "f55aa0bd26b8: Waiting\n",
      "1d0dfb259f6a: Waiting\n",
      "21ec61b65b20: Waiting\n",
      "43c67172d1d1: Waiting\n",
      "f2aae1b4a02a: Mounted from deeplearning-platform-release/base-cpu\n",
      "c9ed5921085c: Mounted from deeplearning-platform-release/base-cpu\n",
      "f0faf772f9d3: Mounted from deeplearning-platform-release/base-cpu\n",
      "4c6f059d1398: Mounted from deeplearning-platform-release/base-cpu\n",
      "079a4b4488d4: Mounted from deeplearning-platform-release/base-cpu\n",
      "801869434e6d: Mounted from deeplearning-platform-release/base-cpu\n",
      "039262d27835: Mounted from deeplearning-platform-release/base-cpu\n",
      "6f8ba6b2269e: Mounted from deeplearning-platform-release/base-cpu\n",
      "448e01508468: Mounted from deeplearning-platform-release/base-cpu\n",
      "67b2302cd884: Mounted from deeplearning-platform-release/base-cpu\n",
      "bb7ac01267bd: Mounted from deeplearning-platform-release/base-cpu\n",
      "34574cdf935e: Pushed\n",
      "f55aa0bd26b8: Layer already exists\n",
      "1d0dfb259f6a: Layer already exists\n",
      "21ec61b65b20: Layer already exists\n",
      "43c67172d1d1: Layer already exists\n",
      "430ca7c3fafc: Mounted from deeplearning-platform-release/base-cpu\n",
      "latest: digest: sha256:4f071a9883f30022afeecbf74a6b718cd2b4b5ac62beee827cd7e4592c6a4f1b size: 3876\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                  IMAGES                                 STATUS\n",
      "a822d5be-d9c6-4ab7-9a42-8392d7c4a0dc  2020-02-18T02:30:16+00:00  3M25S     gs://marinas-demo_cloudbuild/source/1581993015.69-1da9d4434e664a739b7c3f4a4078683f.tgz  gcr.io/marinas-demo/kfp-cli (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME='kfp-cli'\n",
    "IMAGE_URI=\"gcr.io/{}/{}:latest\".format(PROJECT_ID,IMAGE_NAME)\n",
    "!gcloud builds submit --timeout 15m --tag {IMAGE_URI} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "The docker images used for the training are based off the image `mlops-dev:latest` created during the inital set up of the environment. Since the AI Platform Notebook instance is based on the `mlops-dev:latest` image we use the same image as a base for the training image. \n",
    "\n",
    "\n",
    "We first write a base image dockerfile which replicates the image used for the Notebook. Then we write a training dockerfile which uses the same base image and add the `train.py` to the image. \n",
    "\n",
    "\n",
    "**NOTE:** Make sure to update the URI for the image so that it points to your project's **Container Registry**. i.e. `FROM gcr.io/PROJECT_ID/kfp-cli:latest` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline/trainer_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/marinas-demo/mlops-dev:latest\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline/base_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {BASE_IMAGE_FOLDER}/Dockerfile\n",
    "FROM gcr.io/marinas-demo/mlops-dev:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cloud build config file\n",
    "This file is used by **Cloud Build** to create all container images required and deploys the pipeline to kubeflow pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "# Build the trainer image\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/trainer_image\n",
    "  \n",
    "# Build the base image for lightweight components\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/base_image\n",
    "\n",
    "# Compile the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    dsl-compile --py $_PIPELINE_DSL --output $_PIPELINE_PACKAGE\n",
    "  env:\n",
    "  - 'BASE_IMAGE=gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'TRAINER_IMAGE=gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'RUNTIME_VERSION=$_RUNTIME_VERSION'\n",
    "  - 'PYTHON_VERSION=$_PYTHON_VERSION'\n",
    "  - 'COMPONENT_URL_SEARCH_PREFIX=$_COMPONENT_URL_SEARCH_PREFIX'\n",
    "  dir: $_PIPELINE_FOLDER\n",
    "  \n",
    " # Upload the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    kfp --endpoint $_INVERTING_PROXY_HOST pipeline upload -p ${_PIPELINE_NAME}_$TAG_NAME $_PIPELINE_PACKAGE\n",
    "  dir: $_PIPELINE_FOLDER\n",
    "\n",
    "\n",
    "# Push the images to Container Registry \n",
    "images: ['gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', 'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit to Cloud Source Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Source Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME=PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open terminal in Jupyter notebook and set up an authentication channel to cloud source repo. Run the gcloud init command and follow prompts, create a new source repo with the name of your project ID  \n",
    "`gcloud init && git config credential.helper gcloud.sh` - select option 2, then yes, follow the link, copy the verification code back in the terminal command. Once verified, select [1] Enter project ID and type your project ID, then select no for configure region.\n",
    "\n",
    "`gcloud source repos create REPO_NAME` - you will need to specify your REPO_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: remote {REPO_NAME} already exists.\n"
     ]
    }
   ],
   "source": [
    "!cd /home/mlops-demo\n",
    "!git remote add {REPO_NAME} https://source.developers.google.com/p/{PROJECT_NAME}/r/{REPO_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Change email and user name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 6185dc6] Upload ML\n",
      " 1 file changed, 18 insertions(+), 4 deletions(-)\n",
      "Username for 'https://source.developers.google.com': ^C\n"
     ]
    }
   ],
   "source": [
    "!cd /home/mlops-demo\n",
    "!git config --global user.email \"you@example.com\"\n",
    "!git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.name \"marinadeletic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is ahead of 'origin/master' by 2 commits.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "Username for 'https://source.developers.google.com': "
     ]
    }
   ],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git commit -m \"Upload ML\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git push --all {REPO_NAME}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cloud Build trigger\n",
    "This will cause the images to build and will deploy the pipeline apon a push to the source repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a trigger build configuration file\n",
    "This file is used by cloud trigger, to pass the variables required by cloud build. \n",
    "\n",
    "**NOTE** Ensure to change the `Project_ID`, `repoName` and `_INVERTING_PROXY_HOST` variables to reflect your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4bcc62bc46b37077-dot-us-central1.notebooks.googleusercontent.com\n"
     ]
    }
   ],
   "source": [
    "print(INVERSE_PROXY_HOSTNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trigger_build.yaml\n",
    "\n",
    "description: triggers cloud build upon push\n",
    "name: trigger-build\n",
    "filename: 02_demo_CICD/cloudbuild.yaml\n",
    "triggerTemplate:\n",
    "    projectId: marinas-demo\n",
    "    repoName: marinas-demo\n",
    "    branchName: master\n",
    "substitutions:\n",
    "    _INVERTING_PROXY_HOST: 4bcc62bc46b37077-dot-us-central1.notebooks.googleusercontent.com\n",
    "    _TRAINER_IMAGE_NAME: trainer_image\n",
    "    _BASE_IMAGE_NAME: base_image\n",
    "    TAG_NAME: test\n",
    "    _PIPELINE_FOLDER: 02_demo_CICD/pipeline\n",
    "    _PIPELINE_DSL: covertype_training_pipeline.py\n",
    "    _PIPELINE_PACKAGE: covertype_training_pipeline.yaml\n",
    "    _PIPELINE_NAME: covertype_training_deployment\n",
    "    _RUNTIME_VERSION: \"1.14\"\n",
    "    _PYTHON_VERSION: \"3.5\"\n",
    "    _COMPONENT_URL_SEARCH_PREFIX: https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command creates a **Cloud Build** trigger upon any new code committed to the master of our repository. \n",
    "\n",
    "**NOTE**: Only run this command once or it will create multiple triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta builds triggers create cloud-source-repositories --repo=demokfp --branch-pattern=\"master\" --build-config=02_demo_CICD/cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run trigger on push\n",
    "We will now push code to our repository, which will automatically trigger a cloud build of the new container images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"Upload ML\"\n",
    "!git push --all {REPO_NAME}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds list --ongoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} pipeline list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the pipeline\n",
    "The deployed pipeline can be viewed through the Kubeflow Pipeline UI given at the URL below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('https://{}'.format(INVERSE_PROXY_HOSTNAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment \n",
    "Now that the pipeline is deployed we want to run an experiment, this will cause the pipeline to run, pulling the data from bigquery and splitting it, training the models, evaluating them and deploy the best performing model. This experiment takes approximately an hour to execute and will result in a deployed model which can be interacted with through GCP's AI platform predicting service. \n",
    "\n",
    "**NOTE:** Change the PIPELINE_ID to reflect the ID copied from above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='ab90cc6c-3f00-480c-bea3-bd1959e7394b'\n",
    "\n",
    "EXPERIMENT_NAME='Covertype_Classifier_Training'\n",
    "RUN_ID='Run_001'\n",
    "SOURCE_TABLE='covertype_dataset.covertype'\n",
    "DATASET_ID='splits'\n",
    "EVALUATION_METRIC='accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD='0.69'\n",
    "MODEL_ID='covertype_classifier'\n",
    "VERSION_ID='v01'\n",
    "REPLACE_EXISTING_VERSION=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint {INVERSE_PROXY_HOSTNAME} run submit \\\n",
    "-e Covertype_Classifier_Training \\\n",
    "-r {RUN_ID} \\\n",
    "-p {PIPELINE_ID} \\\n",
    "project_id={PROJECT_ID} \\\n",
    "gcs_root={GCS_STAGING_PATH} \\\n",
    "region={REGION} \\\n",
    "source_table_name={SOURCE_TABLE} \\\n",
    "dataset_id={DATASET_ID} \\\n",
    "evaluation_metric_name={EVALUATION_METRIC} \\\n",
    "evaluation_metric_threshold={EVALUATION_METRIC_THRESHOLD} \\\n",
    "model_id={MODEL_ID} \\\n",
    "version_id={VERSION_ID} \\\n",
    "replace_existing_version={REPLACE_EXISTING_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model\n",
    "To test the model we can use the AI platforms prediction API to ask for a prediction based on a JSON input aternatively we can use the prediction UI and input: *{\"instances\":[[2395,0,0,60,6,1170,218,238,156,1054,\"Cache\",\"C2717\"]]}* in the test case window.\n",
    "\n",
    "We write a prediction JSON file with a set of data points, the correct cover types are 3 and 2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predict.json\n",
    "[2395,0,0,60,6,1170,218,238,156,1054,\"Cache\",\"C2717\"]\n",
    "[2756,135,0,85,14,1608,219,238,156,2451,\"Rawah\",\"C4744\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE=\"/home/mlops-demo/predict.json\"\n",
    "\n",
    "!gcloud ai-platform predict --model $MODEL_ID \\\n",
    "  --version $VERSION_ID \\\n",
    "  --json-instances $INPUT_DATA_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r pipeline/ cloudbuild.yaml build_pipeline.sh Dockerfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
